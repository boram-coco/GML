[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graph Machine Learning",
    "section": "",
    "text": "그래프 머신러닝\ngithub\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nApr 28, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(교수님)\n\n\n김보람 \n\n\n\n\nApr 28, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱+그래프)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time-f1:0.009370)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+lat+merch_lat-f1:0.98538)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+city_pop-f1:0.986655)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱amt+time+city_pop+lat+merch_lat-f1:0.985323)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(로지스틱+그래프)\n\n\n김보람 \n\n\n\n\nApr 27, 2023\n\n\nCH8. 신용카드 거래 분석(df원본데이터)\n\n\n김보람 \n\n\n\n\nApr 16, 2023\n\n\nCH8. 신용카드 거래 분석(사기거래=0제외:_df2)\n\n\n김보람 \n\n\n\n\nApr 12, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(under-sampling)\n\n\n김보람 \n\n\n\n\nApr 12, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(over-sampling)\n\n\n김보람 \n\n\n\n\nApr 7, 2023\n\n\nCH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH5. 그래프에서의 머신러닝 문제(링크예측)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH4. 지도 그래프 학습(특징기반방법)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH4. 지도 그래프 학습(얕은 임베딩 방법)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(오토인코더)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(얕은 임베딩 방법)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH3. 비지도 그래프 학습(그래프신경망)\n\n\n김보람 \n\n\n\n\nApr 6, 2023\n\n\nCH2. 그래프 머신러닝\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(undersampling-cluster)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(basic)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(0.4)\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석\n\n\n김보람 \n\n\n\n\nApr 4, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석\n\n\n김보람 \n\n\n\n\nApr 1, 2023\n\n\nCH1. graph basic\n\n\n김보람 \n\n\n\n\nJan 18, 2023\n\n\nCH8. 신용카드 거래에 대한 그래프 분석_코드뜯기\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/graph8.df원본에서진행.html",
    "href": "posts/graph8.df원본에서진행.html",
    "title": "CH8. 신용카드 거래 분석(df원본데이터)",
    "section": "",
    "text": "df에서 그냥 처음부터 샘플뽑지 않고 전체 데이터를 한번 돌려보자. f1-score가 어떻게 나오는지 기존 샘플한거랑 비교해보기\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\n_df[\"is_fraud\"].value_counts()\n\n0    1042569\n1       6006\nName: is_fraud, dtype: int64\n\n\n\n_df[\"is_fraud\"].value_counts()/len(_df)\n\n0    0.994272\n1    0.005728\nName: is_fraud, dtype: float64\n\n\n\ndf = _df\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  2.53it/s]\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7345132743362832\nRecall: 0.1424892703862661\nF1-Score: 0.23867721063982747\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6886792452830188\nRecall: 0.751931330472103\nF1-Score: 0.7189167008617152\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.6136363636363636\nRecall: 0.02317596566523605\nF1-Score: 0.04466501240694789\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.66\nRecall: 0.02832618025751073\nF1-Score: 0.05432098765432099\n\n\n\n음 어차피 나중에 downsampled를 하기 때문에 … f1 score값은 샘플을 무얼 하든 큰 차이가 없다. 오히려 더 낮은듯"
  },
  {
    "objectID": "posts/graph8.사기거래=0필터.html",
    "href": "posts/graph8.사기거래=0필터.html",
    "title": "CH8. 신용카드 거래 분석(사기거래=0제외:_df2)",
    "section": "",
    "text": "df에서 사기거래가 한 번이라도 있었던 사람의 데이터만 모아서 학습시켜봄. 즉, 사기거래가 한번이라도 없던 사람의 데이터는 제거했다.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\ntime\n\n\n\n\n3\n3\n2019-01-01 0:01\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\n...\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n0\n\n\n5\n5\n2019-01-01 0:04\n4.767270e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n94.63\nJennifer\nConner\nF\n4655 David Island\n...\n-75.2045\n2158\nTransport planner\n1961-06-19\n189a841a0a8ba03058526bcfe566aab5\n1325376248\n40.653382\n-76.152667\n0\n0\n\n\n6\n6\n2019-01-01 0:04\n3.007470e+13\nfraud_Rowe-Vandervort\ngrocery_net\n44.54\nKelsey\nRichards\nF\n889 Sarah Station Suite 624\n...\n-100.9893\n2691\nArboriculturist\n1993-08-16\n83ec1cc84142af6e2acf10c44949e720\n1325376282\n37.162705\n-100.153370\n0\n0\n\n\n7\n7\n2019-01-01 0:05\n6.011360e+15\nfraud_Corwin-Collins\ngas_transport\n71.65\nSteven\nWilliams\nM\n231 Flores Pass Suite 720\n...\n-78.6003\n6018\nDesigner, multimedia\n1947-08-21\n6d294ed2cc447d2c71c7171a3d54967c\n1325376308\n38.948089\n-78.540296\n0\n0\n\n\n8\n8\n2019-01-01 0:05\n4.922710e+15\nfraud_Herzog Ltd\nmisc_pos\n4.27\nHeather\nChase\nF\n6888 Hicks Stream Suite 954\n...\n-79.6607\n1472\nPublic affairs consultant\n1941-03-07\nfc28024ce480f8ef21a32d64c93a29f5\n1325376318\n40.351813\n-79.958146\n0\n0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\ndf = _df2\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    \n    return G\n    \n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\nG_tu = build_graph_tripartite(df, nx.Graph())\n\n\nfor G in [G_bu, G_tu]:\n    print(\"nede:\", nx.number_of_nodes(G))\nfor G in [G_bu, G_tu]:\n    print(\"edge:\",nx.number_of_edges(G))\n\nnede: 1289\nnede: 652719\nedge: 268964\nedge: 1302860\n\n\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.37874509971252246\n-0.7222260615980979\n\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n2      21225\n34      9652\n117     9362\n3       8477\n78      8384\n       ...  \n127      812\n110      793\n121      785\n103      449\n95       439\nLength: 142, dtype: int64\n\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n99     2.549511\n69     2.080000\n4      1.996198\n43     1.969697\n133    1.871491\n         ...   \n127    0.246609\n91     0.120192\n104    0.115207\n103    0.000000\n121    0.000000\nLength: 142, dtype: float64\n\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  3.09it/s]\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.5428571428571428\nRecall: 0.016725352112676055\nF1-Score: 0.032450896669513236\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6840981856990395\nRecall: 0.5642605633802817\nF1-Score: 0.6184273999035215\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.45454545454545453\nRecall: 0.022007042253521125\nF1-Score: 0.041981528127623846\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.46296296296296297\nRecall: 0.022007042253521125\nF1-Score: 0.04201680672268907\n\n\n오히려 f1score가 낮아졌다…"
  },
  {
    "objectID": "posts/graph8_.html",
    "href": "posts/graph8_.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지"
  },
  {
    "objectID": "posts/graph8_.html#네트워크-토폴로지",
    "href": "posts/graph8_.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.17,  48.31,  76.29, 146.7 ])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.10159189882353903\n-0.8017506210033467"
  },
  {
    "objectID": "posts/graph8_.html#커뮤니티-감지",
    "href": "posts/graph8_.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n12    4019\n71    3999\n27    3743\n52    3739\n43    3679\n      ... \n32    1110\n93    1097\n49    1060\n26    1003\n33     892\nLength: 96, dtype: int64\n\n\n\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n2500부근에 형성되었고 ..\n\n\ngraphs = []\nd = {}\nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n48    8.684864\n13    6.956522\n55    6.781235\n45    6.743257\n88    6.338616\n        ...   \n93    0.996377\n75    0.952381\n51    0.765957\n82    0.737265\n33    0.335946\nLength: 96, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\n#pip install scipy\n\n\ngId = 48\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8_.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8_.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\n토멕링크\n- 아래는 랜덤다운\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.6953125\nRecall: 0.156140350877193\nF1-Score: 0.2550143266475645\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6813353566009105\nRecall: 0.787719298245614\nF1-Score: 0.7306753458096015\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5925925925925926\nRecall: 0.028070175438596492\nF1-Score: 0.05360134003350084\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.5833333333333334\nRecall: 0.02456140350877193\nF1-Score: 0.04713804713804714\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.25it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.0429862559854\nHomogeneity: 0.03813140300201337\nCompleteness: 0.049433212382250756\nV-Measure: 0.0430529554606017\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.09395128496638593\nHomogeneity: 0.08960753766432715\nCompleteness: 0.09886731281849871\nV-Measure: 0.09400995872350308\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17593048106009063\nHomogeneity: 0.17598531397290276\nCompleteness: 0.17597737533152563\nV-Measure: 0.17598134456268477\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.1362053730791375\nHomogeneity: 0.1349991253997398\nCompleteness: 0.1375429939044335\nV-Measure: 0.13625918760275774"
  },
  {
    "objectID": "posts/graph2.html",
    "href": "posts/graph2.html",
    "title": "CH2. 그래프 머신러닝",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nMachine Learning on Graphs: A Model and Comprehensive Taxonomy\n\n\n\nimport matplotlib.pyplot as plt\ndef draw_graph(G, pos_nodes, node_names={}, node_size=50, plot_weight=False):\n    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray', arrowsize=30)\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n    \n    \n    if plot_weight:\n        pos_attrs = {}\n        for node, coords in pos_nodes.items():\n            pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n        nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n        edge_labels=dict([((a,b,),d[\"weight\"]) for a,b,d in G.edges(data=True)])\n        nx.draw_networkx_edge_labels(G, pos_nodes, edge_labels=edge_labels)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])"
  },
  {
    "objectID": "posts/graph2.html#node2vec-알고리즘-구현-방법",
    "href": "posts/graph2.html#node2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Node2Vec 알고리즘 구현 방법",
    "text": "Node2Vec 알고리즘 구현 방법\n1 그래프 생성\n2 Node2Vec 객체 생성\nnode2vec = Node2Vec(G, dimensions=, walk_length=, num_walks=, p=, q=, workers=)\n\ndimensions : 임베딩할 벡터의 차원 수\nwalk_length : 랜덤 워크에서 한 번에 이동할 노드 수\nnum_walks : 랜덤 워크를 몇 번 반복할 것인지\np, q : 노드 탐색을 위한 확률값을 조정하는 매개변수\n\n3 랜덤 워크를 수행하여 노드 시퀀스 생성\n4 생성된 노드 시퀀스(그래프 내의 노드들이 순서) 임베딩\n\n랜덤 워크(Random Walk)\n\n그래프 내의 노드를 방문하며 노드 간의 관계를 탐색하는 과정\n시작 노드에서 출발하여 현재 노드와 연결된 노드들 중 하나를 무작위로 선택하여 이동\n만약 선택된 노드가 이전에 방문한 노드이면 return parameter에 따라 이전 노드로 다시 돌아갈 확률이 높아짐\n반면, 이전 노드와 멀리 떨어진 노드를 샘플링하는 경우에는 in-out parameter에 따라 더 멀리 떨어진 노드를 샘플링하는 비율이 조절\n랜덤 워크 시퀀스는 각 노드의 방문 횟수, 이웃 노드의 구성 등 그래프의 구조적 특성을 반영"
  },
  {
    "objectID": "posts/graph2.html#node2vec-example",
    "href": "posts/graph2.html#node2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Node2Vec example",
    "text": "Node2Vec example\n\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=7, m2=4)  # 바벨 그래프 생성\ndraw_graph(G, nx.spring_layout(G))\n\nnode2vec = Node2Vec(G, dimensions=2) # 그래프의 각 노드를 2차원 벡터로 생성\nmodel = node2vec.fit(window=10) # 원본 그래프 노드에 임베딩 알고리즘 적용해 생성된 2차원 벡터\n     \n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:00&lt;00:00, 346.40it/s]"
  },
  {
    "objectID": "posts/graph2.html#edge2vec-알고리즘-구현-방법",
    "href": "posts/graph2.html#edge2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Edge2Vec 알고리즘 구현 방법",
    "text": "Edge2Vec 알고리즘 구현 방법\n1 그래프 데이터 로딩\n2 모델 학습\n3 엣지 임베딩: 학습된 Edge2Vec모델을 이용해 각 엣지를 임베딩 벡터로 변환하고 임베딩 벡터의 차원 수는 모델 하이퍼파라미터 값에 따른다.\n4 분류 또는 유사도 측정"
  },
  {
    "objectID": "posts/graph2.html#edge2vec-example",
    "href": "posts/graph2.html#edge2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Edge2Vec example",
    "text": "Edge2Vec example\n\nfrom node2vec.edges import HadamardEmbedder # Node2Vec 모델을 사용하여 학습된 그래프 임베딩 모델에서 HadamardEmbedder를 이용하여 엣지 임베딩 벡터를 생성하고, 그것을 시각화하는 코드\n\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) # model.wv : 학습된 노드 입베딩 벡터를 가지고있는 객체\nfig, ax=plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)  # 엣지를 해당하는 점위에 텍스트로 나타냄, 즉 해당 점의 x,y좌표를 설정\n\n\n\n\n\n\n\n\n\nHadamardEmbedder: 두 노드의 임베딩 벡터를 element-wise 곱한 결과를 사용하여 엣지의 임베딩 벡터를 생성"
  },
  {
    "objectID": "posts/graph2.html#graph2vec-알고리즘-구현-방법",
    "href": "posts/graph2.html#graph2vec-알고리즘-구현-방법",
    "title": "CH2. 그래프 머신러닝",
    "section": "Graph2Vec 알고리즘 구현 방법",
    "text": "Graph2Vec 알고리즘 구현 방법\n1 그래프 분할: 서로 겹치지 않게 입력 그래프를 여러 개의 부분 그래프로 분할\n2 순서 부여: 분할된 부분 그래프에 순서를 부여\n3 부분 그래프 임베딩\n4 전체 그래프 임베딩\n5 분류 또는 유사도 측정"
  },
  {
    "objectID": "posts/graph2.html#graph2vec-example",
    "href": "posts/graph2.html#graph2vec-example",
    "title": "CH2. 그래프 머신러닝",
    "section": "Graph2Vec Example",
    "text": "Graph2Vec Example\n\npip install karateclub\n\nCollecting karateclub\n  Downloading karateclub-1.3.3.tar.gz (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.5/64.5 kB 5.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nCollecting numpy&lt;1.23.0\n  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 81.5 MB/s eta 0:00:0000:0100:01\nCollecting networkx&lt;2.7\n  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 88.6 MB/s eta 0:00:00\nCollecting decorator==4.4.2\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: tqdm in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (4.65.0)\nRequirement already satisfied: python-louvain in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (0.16)\nRequirement already satisfied: scikit-learn in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.2.2)\nRequirement already satisfied: scipy in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.10.1)\nCollecting pygsp\n  Downloading PyGSP-0.5.1-py2.py3-none-any.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 78.9 MB/s eta 0:00:00\nRequirement already satisfied: gensim&gt;=4.0.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (4.3.1)\nCollecting pandas&lt;=1.3.5\n  Downloading pandas-1.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 94.0 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: six in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from karateclub) (1.16.0)\nCollecting python-Levenshtein\n  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: smart-open&gt;=1.8.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&gt;=4.0.0-&gt;karateclub) (6.3.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas&lt;=1.3.5-&gt;karateclub) (2022.7.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas&lt;=1.3.5-&gt;karateclub) (2.8.2)\nCollecting Levenshtein==0.20.9\n  Downloading Levenshtein-0.20.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174.0/174.0 kB 36.6 MB/s eta 0:00:00\nCollecting rapidfuzz&lt;3.0.0,&gt;=2.3.0\n  Downloading rapidfuzz-2.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 102.2 MB/s eta 0:00:00\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn-&gt;karateclub) (3.1.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn-&gt;karateclub) (1.2.0)\nBuilding wheels for collected packages: karateclub\n  Building wheel for karateclub (setup.py) ... done\n  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101987 sha256=9b0452b9dfbfaa045ccbf70aeee54c5ac34fdb4bf2d2af2d6facab840d6d499d\n  Stored in directory: /home/coco/.cache/pip/wheels/2b/93/72/8e0b3ec687bea23bd34bbb723a82fcb6b074cb756a29441f0c\nSuccessfully built karateclub\nInstalling collected packages: rapidfuzz, numpy, networkx, decorator, pandas, Levenshtein, python-Levenshtein, pygsp, karateclub\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.2\n    Uninstalling numpy-1.24.2:\n      Successfully uninstalled numpy-1.24.2\n  Attempting uninstall: networkx\n    Found existing installation: networkx 2.8.8\n    Uninstalling networkx-2.8.8:\n      Successfully uninstalled networkx-2.8.8\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Uninstalling decorator-5.1.1:\n      Successfully uninstalled decorator-5.1.1\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\nSuccessfully installed Levenshtein-0.20.9 decorator-4.4.2 karateclub-1.3.3 networkx-2.6.3 numpy-1.23.5 pandas-1.3.5 pygsp-0.5.1 python-Levenshtein-0.20.9 rapidfuzz-2.15.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport random\nimport matplotlib.pyplot as plt\n\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)  # 노드 갯수\n    k = random.randint(5, n)   # k-최근접 이웃 개수\n    p = random.uniform(0, 1)   # 연결 확률 파라미터\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]   # 함수를 사용하여 20개의 무작위 그래프 생성\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10)  # 임베딩 벡터 차원수: 2차원\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/graph2.html#얕은-임베딩",
    "href": "posts/graph2.html#얕은-임베딩",
    "title": "CH2. 그래프 머신러닝",
    "section": "얕은 임베딩",
    "text": "얕은 임베딩\n\n학습된 입력 데이터에 대한 임베딩 값만 학습하고 반환\nNode2Vec, Edge2Vec, Graph2Vec\n학습한 데이터의 벡터 표현만 반환\n보이지 않는 데이터에 대한 임베딩 벡터는 못얻는다.\n지도학습/비지도학습 각각 정의 가능\n\n\nmodel.fit(graphs_list) #graphs_list 학습\nembedding=model.get_embedding()[i]"
  },
  {
    "objectID": "posts/graph2.html#그래프-자동-인코딩",
    "href": "posts/graph2.html#그래프-자동-인코딩",
    "title": "CH2. 그래프 머신러닝",
    "section": "그래프 자동 인코딩",
    "text": "그래프 자동 인코딩\n\n보이지 않는 인스턴스에 대한 임베딩 벡터도 생성\n비지도 학습에 적합\n\n\nmodel.fit(graphs_list) #graphs_list 학습\nembedding=model.get_embedding(G) #보이지 않는 새로운 그래프 G의 임베딩 벡터 생성"
  },
  {
    "objectID": "posts/graph2.html#근방-집계",
    "href": "posts/graph2.html#근방-집계",
    "title": "CH2. 그래프 머신러닝",
    "section": "근방 집계",
    "text": "근방 집계\n\n그래프 수준에서 임베딩 추출\n노드는 일부 소석승 기반으로 라벨 지정\n일반 사상 함수 \\(f(G)\\)를 학습 할 수 있고 보이지 않는 인스턴스에 대한 임베딩 벡터도 생성 가능"
  },
  {
    "objectID": "posts/graph2.html#그래프-정규화",
    "href": "posts/graph2.html#그래프-정규화",
    "title": "CH2. 그래프 머신러닝",
    "section": "그래프 정규화",
    "text": "그래프 정규화\n\n그래프를 입력으로 받지 않는다.\n프로세스를 정규화하고자 상호작용을 나타내는 특정집합 학습\n준지도학습/지도 학습에 사용"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html",
    "href": "posts/graph8(frac=0.3).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "href": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "href": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.47it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7349397590361446\nRecall: 0.15996503496503497\nF1-Score: 0.26274228284278534\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6856264411990777\nRecall: 0.7797202797202797\nF1-Score: 0.7296523517382413\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5737704918032787\nRecall: 0.030594405594405596\nF1-Score: 0.05809128630705394\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.609375\nRecall: 0.03409090909090909\nF1-Score: 0.06456953642384106\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph5-2.html",
    "href": "posts/graph5-2.html",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph5-2.html#임베딩-기반-커뮤니티-감지",
    "href": "posts/graph5-2.html#임베딩-기반-커뮤니티-감지",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "임베딩 기반 커뮤니티 감지",
    "text": "임베딩 기반 커뮤니티 감지\n\n노드 임베딩에 얕은 클러스터링 기술 적용\n임베딩 방법 사용시 노드 간의 유사성을 나타내는 거리를 정의하는 벡터 공간에 노드를 투영\n\n\n바벨 그래프 생성\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n\nimport networkx as nx \nG = nx.barbell_graph(m1=10, m2=4) \n\n\n임베딩 알고리즘(HOPE)를 사용해 감소된 밀집 노드 얻기\n\n\nfrom gem.embedding.hope import HOPE \ngf = HOPE(d=4, beta=0.01) \ngf.learn_embedding(G) \nembeddings = gf.get_embedding() \n\nSVD error (low rank): 0.052092\n\n\n\n클러스터링 알고리즘 실행\n\n\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=3, random_state=0) #.(embeddings)\nlabels = gm.fit_predict(embeddings)\n\n\n다양한 색상으로 강조 표시된 계산된 커뮤니티로 네트워크 그리기\n\ncolors = [\"blue\", \"green\", \"red\"]\n\nnx.draw_spring(G, node_color=[colors[label] for label in labels])\n\n위 코드 자꾸 '_AxesStack' object is not callable 이렇게 오류가 난다. ㅇ\n아래와 같이 그림이 표시되어야 함"
  },
  {
    "objectID": "posts/graph5-2.html#스펙트럼-방법-및-행렬-분해",
    "href": "posts/graph5-2.html#스펙트럼-방법-및-행렬-분해",
    "title": "CH5. 그래프에서의 머신러닝 문제(커뮤니티와 같은 의미 있는 구조 감지)",
    "section": "스펙트럼 방법 및 행렬 분해",
    "text": "스펙트럼 방법 및 행렬 분해\n\n스펙트럼 클러스터링: 라플라시안 행렬의 고유 벡터에 표준 클러스터링 알고리즘 적용\n임베딩 기술이 라플라시안 행렬의 첫 번쨰 k-고유 벡터롤 고려해 얻은 스펙트럼 임베딩을 이용한 임베딩 기반 커뮤니티 탐지 알고리즘의 특별한 경우 (?)\n\n\n#pip install communities\n\n\nfrom communities.algorithms import spectral_clustering\n\nadj=np.array(nx.adjacency_matrix(G).todense())\n\ncommunities = spectral_clustering(adj, k=3)\n\n/tmp/ipykernel_1863787/1958540878.py:3: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  adj=np.array(nx.adjacency_matrix(G).todense())\n\n\n\nplt.figure(figsize=(20, 5))\n\nfor ith, community in enumerate(communities):\n    cols = [\"red\" if node in community else \"blue\" for node in G.nodes]\n    plt.subplot(1,3,ith+1)\n    plt.title(f\"Community {ith}\")\n    nx.draw_spring(G, node_color=cols)\n\nTypeError: '_AxesStack' object is not callable\n\n\n\n\n\n\n\n\n\n\n그전에 nx.draw가 오류 날때는 nx.draw_networkx로 바꿔서 하니까 되긴 됬는데.. nx.darw_spinrg은 어떻게 해야하누.."
  },
  {
    "objectID": "posts/graph8_코드뜯기.html",
    "href": "posts/graph8_코드뜯기.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석_코드뜯기",
    "section": "",
    "text": "ref\n\n그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\n\n\nimport pandas as pd\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n/tmp/ipykernel_3113237/372253127.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n총 265,342건 거래 중 7,506건(2,83%)가 사기\n\n\n\n이분그래프\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n- 이분그래프\n\ndf_ = df.copy()\nmapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \ndf[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\ndf[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n\nfrom은 cc_num 고유의 값이라고 생각. to의 값도 merkchant의 고유값\n\n943+693 # 고객  + 상점\n\n1636\n\n\n\ndf[df['from'] == 1]\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\nfrom\nto\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n1\n924\n\n\n1019480\n1019480\n2020-02-24 22:42\n4.089100e+18\nfraud_Boyer PLC\nshopping_net\n7.11\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\nfafe649e0bc55f131168b2d9dd84463a\n1361745777\n33.363174\n-94.943839\n0\n1\n415\n\n\n332666\n332666\n2019-06-07 15:03\n4.089100e+18\nfraud_Stiedemann Ltd\nfood_dining\n106.83\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\na7bbe4b43fcb572f950109bece88ce26\n1339081432\n32.376099\n-94.801647\n0\n1\n1500\n\n\n665008\n665008\n2019-10-10 17:11\n4.089100e+18\nfraud_Altenwerth-Kilback\nhome\n89.68\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\n1e5f07116dcc5a4fa062168987c121a1\n1349889093\n31.410590\n-95.486031\n0\n1\n42\n\n\n417225\n417225\n2019-07-07 11:24\n4.089100e+18\nfraud_Koepp-Witting\ngrocery_pos\n174.12\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\n68ede1422d7d6fe0b435661c75cccaa9\n1341660240\n32.792707\n-94.622866\n0\n1\n1424\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n907914\n907914\n2019-12-28 18:05\n4.089100e+18\nfraud_Hagenes, Kohler and Hoppe\nfood_dining\n31.66\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\na339b4ff5619129e30d0914fb264c6b2\n1356717951\n32.075614\n-95.385633\n0\n1\n1003\n\n\n545087\n545087\n2019-08-21 14:25\n4.089100e+18\nfraud_Thiel Ltd\ntravel\n2.59\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\n1b72cd11da52010b7f751de70621d68e\n1345559124\n31.514842\n-94.502117\n0\n1\n666\n\n\n890185\n890185\n2019-12-23 22:48\n4.089100e+18\nfraud_Hyatt, Russel and Gleichner\nhealth_fitness\n156.26\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\n772bdb76fb8e3a365864899d1b7b3a77\n1356302914\n33.072255\n-95.310844\n0\n1\n1019\n\n\n505273\n505273\n2019-08-07 12:33\n4.089100e+18\nfraud_Yost, Schamberger and Windler\nkids_pets\n114.13\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\n2cdfcd476e3b08f32a78190c1268df55\n1344342827\n32.321521\n-95.143493\n0\n1\n1139\n\n\n253317\n253317\n2019-05-06 8:09\n4.089100e+18\nfraud_Rempel PLC\ngrocery_net\n52.49\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\na807c5c9e853e94ec7b1680eed9d46c5\n1336291776\n33.103470\n-95.157733\n0\n1\n814\n\n\n\n\n251 rows × 25 columns\n\n\n\n\ndf = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n\n\ndf\n\n\n\n\n\n\n\n\nfrom\nto\nis_fraud\namt\n\n\n\n\n0\n0\n12\n0\n92.67\n\n\n1\n0\n14\n0\n91.26\n\n\n2\n0\n19\n0\n35.77\n\n\n3\n0\n27\n0\n138.70\n\n\n4\n0\n28\n0\n52.75\n\n\n...\n...\n...\n...\n...\n\n\n169967\n1635\n1460\n0\n50.63\n\n\n169968\n1635\n1572\n0\n13.33\n\n\n169969\n1635\n1581\n0\n160.64\n\n\n169970\n1635\n1598\n0\n8.35\n\n\n169971\n1635\n1616\n1\n71.78\n\n\n\n\n169972 rows × 4 columns\n\n\n\n\n#위와 같은 예시\n\ndata = {\n    'from': ['A', 'B', 'A', 'B', 'A'],\n    'to': ['X', 'Y', 'X', 'Y', 'Z'],\n    'amt': [100, 200, 150, 300, 120],\n    'is_fraud': [0, 1, 0, 1, 0]\n}\n\ndf = pd.DataFrame(data)\n\ndf_grouped = df[['from', 'to', 'amt', 'is_fraud']].groupby(['from', 'to']).agg({\"is_fraud\": \"sum\", \"amt\": \"sum\"}).reset_index()\n\nprint(df_grouped)\n\n  from to  is_fraud  amt\n0    A  X         0  250\n1    A  Z         0  120\n2    B  Y         2  500\n\n\n\ndf[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n\n\ndf\n\n\n\n\n\n\n\n\nfrom\nto\nis_fraud\namt\n\n\n\n\n0\n0\n12\n0\n92.67\n\n\n1\n0\n14\n0\n91.26\n\n\n2\n0\n19\n0\n35.77\n\n\n3\n0\n27\n0\n138.70\n\n\n4\n0\n28\n0\n52.75\n\n\n...\n...\n...\n...\n...\n\n\n169967\n1635\n1460\n0\n50.63\n\n\n169968\n1635\n1572\n0\n13.33\n\n\n169969\n1635\n1581\n0\n160.64\n\n\n169970\n1635\n1598\n0\n8.35\n\n\n169971\n1635\n1616\n1\n71.78\n\n\n\n\n169972 rows × 4 columns\n\n\n\n\nG=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=nx.Graph())\n\n\nG\n\n&lt;networkx.classes.graph.Graph at 0x7fa4e2c0cb20&gt;\n\n\n\nG?\n\n\nType:           Graph\nString form:    Graph with 1636 nodes and 169972 edges\nLength:         1636\nFile:           ~/anaconda3/envs/py38/lib/python3.8/site-packages/networkx/classes/graph.py\nDocstring:     \nBase class for undirected graphs.\nA Graph stores nodes and edges with optional data, or attributes.\nGraphs hold undirected edges.  Self loops are allowed but multiple\n(parallel) edges are not.\nNodes can be arbitrary (hashable) Python objects with optional\nkey/value attributes, except that `None` is not allowed as a node.\nEdges are represented as links between nodes with optional\nkey/value attributes.\nParameters\n----------\nincoming_graph_data : input graph (optional, default: None)\n    Data to initialize graph. If None (default) an empty\n    graph is created.  The data can be any format that is supported\n    by the to_networkx_graph() function, currently including edge list,\n    dict of dicts, dict of lists, NetworkX graph, 2D NumPy array, SciPy\n    sparse matrix, or PyGraphviz graph.\nattr : keyword arguments, optional (default= no attributes)\n    Attributes to add to graph as key=value pairs.\nSee Also\n--------\nDiGraph\nMultiGraph\nMultiDiGraph\nExamples\n--------\nCreate an empty graph structure (a \"null graph\") with no nodes and\nno edges.\n&gt;&gt;&gt; G = nx.Graph()\nG can be grown in several ways.\n**Nodes:**\nAdd one node at a time:\n&gt;&gt;&gt; G.add_node(1)\nAdd the nodes from any container (a list, dict, set or\neven the lines from a file or the nodes from another graph).\n&gt;&gt;&gt; G.add_nodes_from([2, 3])\n&gt;&gt;&gt; G.add_nodes_from(range(100, 110))\n&gt;&gt;&gt; H = nx.path_graph(10)\n&gt;&gt;&gt; G.add_nodes_from(H)\nIn addition to strings and integers any hashable Python object\n(except None) can represent a node, e.g. a customized node object,\nor even another Graph.\n&gt;&gt;&gt; G.add_node(H)\n**Edges:**\nG can also be grown by adding edges.\nAdd one edge,\n&gt;&gt;&gt; G.add_edge(1, 2)\na list of edges,\n&gt;&gt;&gt; G.add_edges_from([(1, 2), (1, 3)])\nor a collection of edges,\n&gt;&gt;&gt; G.add_edges_from(H.edges)\nIf some edges connect nodes not yet in the graph, the nodes\nare added automatically.  There are no errors when adding\nnodes or edges that already exist.\n**Attributes:**\nEach graph, node, and edge can hold key/value attribute pairs\nin an associated attribute dictionary (the keys must be hashable).\nBy default these are empty, but can be added or changed using\nadd_edge, add_node or direct manipulation of the attribute\ndictionaries named graph, node and edge respectively.\n&gt;&gt;&gt; G = nx.Graph(day=\"Friday\")\n&gt;&gt;&gt; G.graph\n{'day': 'Friday'}\nAdd node attributes using add_node(), add_nodes_from() or G.nodes\n&gt;&gt;&gt; G.add_node(1, time=\"5pm\")\n&gt;&gt;&gt; G.add_nodes_from([3], time=\"2pm\")\n&gt;&gt;&gt; G.nodes[1]\n{'time': '5pm'}\n&gt;&gt;&gt; G.nodes[1][\"room\"] = 714  # node must exist already to use G.nodes\n&gt;&gt;&gt; del G.nodes[1][\"room\"]  # remove attribute\n&gt;&gt;&gt; list(G.nodes(data=True))\n[(1, {'time': '5pm'}), (3, {'time': '2pm'})]\nAdd edge attributes using add_edge(), add_edges_from(), subscript\nnotation, or G.edges.\n&gt;&gt;&gt; G.add_edge(1, 2, weight=4.7)\n&gt;&gt;&gt; G.add_edges_from([(3, 4), (4, 5)], color=\"red\")\n&gt;&gt;&gt; G.add_edges_from([(1, 2, {\"color\": \"blue\"}), (2, 3, {\"weight\": 8})])\n&gt;&gt;&gt; G[1][2][\"weight\"] = 4.7\n&gt;&gt;&gt; G.edges[1, 2][\"weight\"] = 4\nWarning: we protect the graph data structure by making `G.edges` a\nread-only dict-like structure. However, you can assign to attributes\nin e.g. `G.edges[1, 2]`. Thus, use 2 sets of brackets to add/change\ndata attributes: `G.edges[1, 2]['weight'] = 4`\n(For multigraphs: `MG.edges[u, v, key][name] = value`).\n**Shortcuts:**\nMany common graph features allow python syntax to speed reporting.\n&gt;&gt;&gt; 1 in G  # check if node in graph\nTrue\n&gt;&gt;&gt; [n for n in G if n &lt; 3]  # iterate through nodes\n[1, 2]\n&gt;&gt;&gt; len(G)  # number of nodes in graph\n5\nOften the best way to traverse all edges of a graph is via the neighbors.\nThe neighbors are reported as an adjacency-dict `G.adj` or `G.adjacency()`\n&gt;&gt;&gt; for n, nbrsdict in G.adjacency():\n...     for nbr, eattr in nbrsdict.items():\n...         if \"weight\" in eattr:\n...             # Do something useful with the edges\n...             pass\nBut the edges() method is often more convenient:\n&gt;&gt;&gt; for u, v, weight in G.edges.data(\"weight\"):\n...     if weight is not None:\n...         # Do something useful with the edges\n...         pass\n**Reporting:**\nSimple graph information is obtained using object-attributes and methods.\nReporting typically provides views instead of containers to reduce memory\nusage. The views update as the graph is updated similarly to dict-views.\nThe objects `nodes`, `edges` and `adj` provide access to data attributes\nvia lookup (e.g. `nodes[n]`, `edges[u, v]`, `adj[u][v]`) and iteration\n(e.g. `nodes.items()`, `nodes.data('color')`,\n`nodes.data('color', default='blue')` and similarly for `edges`)\nViews exist for `nodes`, `edges`, `neighbors()`/`adj` and `degree`.\nFor details on these and other miscellaneous methods, see below.\n**Subclasses (Advanced):**\nThe Graph class uses a dict-of-dict-of-dict data structure.\nThe outer dict (node_dict) holds adjacency information keyed by node.\nThe next dict (adjlist_dict) represents the adjacency information and holds\nedge data keyed by neighbor.  The inner dict (edge_attr_dict) represents\nthe edge data and holds edge attribute values keyed by attribute names.\nEach of these three dicts can be replaced in a subclass by a user defined\ndict-like object. In general, the dict-like features should be\nmaintained but extra features can be added. To replace one of the\ndicts create a new graph class by changing the class(!) variable\nholding the factory for that dict-like structure.\nnode_dict_factory : function, (default: dict)\n    Factory function to be used to create the dict containing node\n    attributes, keyed by node id.\n    It should require no arguments and return a dict-like object\nnode_attr_dict_factory: function, (default: dict)\n    Factory function to be used to create the node attribute\n    dict which holds attribute values keyed by attribute name.\n    It should require no arguments and return a dict-like object\nadjlist_outer_dict_factory : function, (default: dict)\n    Factory function to be used to create the outer-most dict\n    in the data structure that holds adjacency info keyed by node.\n    It should require no arguments and return a dict-like object.\nadjlist_inner_dict_factory : function, (default: dict)\n    Factory function to be used to create the adjacency list\n    dict which holds edge data keyed by neighbor.\n    It should require no arguments and return a dict-like object\nedge_attr_dict_factory : function, (default: dict)\n    Factory function to be used to create the edge attribute\n    dict which holds attribute values keyed by attribute name.\n    It should require no arguments and return a dict-like object.\ngraph_attr_dict_factory : function, (default: dict)\n    Factory function to be used to create the graph attribute\n    dict which holds attribute values keyed by attribute name.\n    It should require no arguments and return a dict-like object.\nTypically, if your extension doesn't impact the data structure all\nmethods will inherit without issue except: `to_directed/to_undirected`.\nBy default these methods create a DiGraph/Graph class and you probably\nwant them to create your extension of a DiGraph/Graph. To facilitate\nthis we define two class variables that you can set in your subclass.\nto_directed_class : callable, (default: DiGraph or MultiDiGraph)\n    Class to create a new graph structure in the `to_directed` method.\n    If `None`, a NetworkX class (DiGraph or MultiDiGraph) is used.\nto_undirected_class : callable, (default: Graph or MultiGraph)\n    Class to create a new graph structure in the `to_undirected` method.\n    If `None`, a NetworkX class (Graph or MultiGraph) is used.\n**Subclassing Example**\nCreate a low memory graph class that effectively disallows edge\nattributes by using a single attribute dict for all edges.\nThis reduces the memory used, but you lose edge attributes.\n&gt;&gt;&gt; class ThinGraph(nx.Graph):\n...     all_edge_dict = {\"weight\": 1}\n...\n...     def single_edge_dict(self):\n...         return self.all_edge_dict\n...\n...     edge_attr_dict_factory = single_edge_dict\n&gt;&gt;&gt; G = ThinGraph()\n&gt;&gt;&gt; G.add_edge(2, 1)\n&gt;&gt;&gt; G[2][1]\n{'weight': 1}\n&gt;&gt;&gt; G.add_edge(2, 2)\n&gt;&gt;&gt; G[2][1] is G[2][2]\nTrue\nInit docstring:\nInitialize a graph with edges, name, or graph attributes.\nParameters\n----------\nincoming_graph_data : input graph (optional, default: None)\n    Data to initialize graph. If None (default) an empty\n    graph is created.  The data can be an edge list, or any\n    NetworkX graph object.  If the corresponding optional Python\n    packages are installed the data can also be a 2D NumPy array, a\n    SciPy sparse array, or a PyGraphviz graph.\nattr : keyword arguments, optional (default= no attributes)\n    Attributes to add to graph as key=value pairs.\nSee Also\n--------\nconvert\nExamples\n--------\n&gt;&gt;&gt; G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc\n&gt;&gt;&gt; G = nx.Graph(name=\"my graph\")\n&gt;&gt;&gt; e = [(1, 2), (2, 3), (3, 4)]  # list of edges\n&gt;&gt;&gt; G = nx.Graph(e)\nArbitrary graph attribute pairs (key=value) may be assigned\n&gt;&gt;&gt; G = nx.Graph(e, day=\"Friday\")\n&gt;&gt;&gt; G.graph\n{'day': 'Friday'}\n\n\n\n\nnx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n\n\nedge의 특성 label에 사기거래 여부를 넣음\n\n\nnx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n\nedge의 특성 weight에 거래금액을 넣음\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\nG_bu = build_graph_bipartite(df_, nx.Graph(name=\"Bipartite Undirect\"))\n\n\n\n삼분그래프\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    \n    return G\n    \n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n\n/tmp/ipykernel_3113237/1475241791.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n\n\n\nmapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n\n\ndf[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\ndf[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\nin_node\nout_node\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n128931\n164764\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n88221\n132732\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n46263\n10634\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n201961\n212159\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n163599\n69533\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1047089\n1047089\n2020-03-10 3:59\n3.589290e+15\nfraud_Kris-Weimann\nmisc_net\n690.49\nPaula\nEstrada\nF\n350 Stacy Glens\n...\n343\nDevelopment worker, international aid\n1972-03-05\nfb1ddd251bbec9b84c9755e856d51723\n1362887989\n43.254214\n-98.267759\n1\n58387\n171403\n\n\n1047157\n1047157\n2020-03-10 4:31\n3.546670e+15\nfraud_Casper, Hand and Zulauf\ngrocery_pos\n324.74\nJordan\nMay\nM\n1626 Susan Course\n...\n13602\nOptometrist\n1984-07-05\n4dca0549e43b7e265cae7fd8a7e563b4\n1362889904\n33.607221\n-97.996506\n1\n116986\n152186\n\n\n1047208\n1047208\n2020-03-10 4:59\n3.589290e+15\nfraud_Kiehn Inc\ngrocery_pos\n331.33\nPaula\nEstrada\nF\n350 Stacy Glens\n...\n343\nDevelopment worker, international aid\n1972-03-05\nd18c55035998e461aa9040e254b74925\n1362891561\n44.228731\n-98.330520\n1\n58387\n30872\n\n\n1047521\n1047521\n2020-03-10 8:22\n3.589290e+15\nfraud_Rau and Sons\ngrocery_pos\n356.20\nPaula\nEstrada\nF\n350 Stacy Glens\n...\n343\nDevelopment worker, international aid\n1972-03-05\nbdaeb5e3413a408d7e6c3720a35337d5\n1362903771\n43.988931\n-97.989985\n1\n58387\n207092\n\n\n1047918\n1047918\n2020-03-10 12:09\n3.589290e+15\nfraud_O'Connell-Ullrich\nhome\n249.56\nPaula\nEstrada\nF\n350 Stacy Glens\n...\n343\nDevelopment worker, international aid\n1972-03-05\n8f0bac74e340483b44babb0d6d07b85b\n1362917373\n42.868322\n-98.537668\n1\n58387\n161788\n\n\n\n\n214520 rows × 25 columns\n\n\n\n\n G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=nx.Graph())\n\n\nnx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\nnx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n\n\nlabel에 사기 거래 여부 표시\n\n\nnx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \nnx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    \n    return G\n    \n\n\nG_tu = build_graph_tripartite(df, nx.Graph())\n\n\n\n지도학습(이분그래프)\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()      \ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n\n질문! 왜 왜 ………….. G_dow.nodes - train_grahp.nodes 를 하는거지?\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7285714285714285\nRecall: 0.13144329896907217\nF1-Score: 0.22270742358078602\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6959247648902821\nRecall: 0.7628865979381443\nF1-Score: 0.7278688524590164\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.574468085106383\nRecall: 0.023195876288659795\nF1-Score: 0.04459124690338563\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.5319148936170213\nRecall: 0.02147766323024055\nF1-Score: 0.041288191577208914\n\n\n.wv : 단어 벡터\n만약\nedgs = [(1, 2), (2, 3), (3, 4), (4, 5)]\nx=2라면 이라면, edge[x=2][0] = 3 edge[x=2][1] = 4 이다.\n- 임베딩 방법(ChatGTP)\nDeepWalk: 노드의 임베딩을 학습하기 위해 Skip-gram 방식의 Word2Vec을 그래프에 적용하는 방법입니다.\nGraphSAGE (Graph Sample and Aggregated Embedding): 각 노드에 대해 이웃 노드들의 임베딩을 집계하여 해당 노드의 임베딩을 생성하는 방법입니다.\nGraph Attention Networks (GAT): 노드 간의 관계를 고려한 그래프 신경망으로, 노드에 대한 임베딩을 학습할 수 있습니다.\nTADW (Topological Deep Walk): 토플로지와 텍스트 정보를 결합하여 노드를 임베딩하는 방법입니다.\n\n\n지도학습(삼분그래프)\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_tripartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()      \ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:24&lt;00:00,  2.42s/it]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred))"
  },
  {
    "objectID": "posts/graph5-1.html",
    "href": "posts/graph5-1.html",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph5-1.html#유사성-기반-방법",
    "href": "posts/graph5-1.html#유사성-기반-방법",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "유사성 기반 방법",
    "text": "유사성 기반 방법\n\n지수 기반 방법\n\n주어진 두 노드의 이웃을 기반으로 간단한 지수 계싼을 통한 방법\n\n\n자원 할당 지수\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.resource_allocation_index(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\n\n[(1, 2, 0.5), (2, 5, 0.5), (3, 4, 0.5)]\n\n\n\n노드 쌍의 지원 할당 지수 목록\n노드 쌍 사이에 간선이 있을 확률 0.5\n\n- 오류나넹.. 아래와 같이 코드 나와야함\n\n\ndraw_graph(G)\n\n\n\n자카드 계수\n\\[jaccard Coefficient(u,v) = \\dfrac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}\\]\n\n\\(N(v)\\):노드의 이웃 계산\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.jaccard_coefficient(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n\nTypeError: '_AxesStack' object is not callable\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n- 위 함수는 자꾸 오류가 나서 밑에 처럼 바꿔서 진행\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef draw_graph(G):\n    nx.draw_networkx(G, with_labels=True)\n    plt.show()\n\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.jaccard_coefficient(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n\n\n\n\n\n\n\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n노드 (1,2)사이 간선 확률 0.5, (2,5)사이에 간선 확률 0.25, (3,4)사이에 간선 확률 0.3\n\n\n\n\n커뮤니티 기반 방법\n\n주어진 두 노드가 속한 커뮤니티에 대한 정보를 사용해 지수 계산\n\n\n커뮤니티 공통 이웃\n\n공통 이웃 수를 계산하고 이 값에 동일한 커뮤니티에 속한 공통 이웃 수를 더한다.\n\n\\[Community Common Neighbor(u,v)=|N(v) \\cup N(u)| + \\sum_{w in N(v) \\cap N(u)} f(w)\\]\n\nw가 u,v와 동일한 커뮤니티에 속하면 \\(f(w)=1\\) 그렇지 않으면 \\(0\\)\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\n\n# 1,2,3은 0이라는 커뮤니티\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\n\n\n# 4,5,6,7은 1이라는 커뮤니티\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\n\n\npreds = nx.cn_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\nnx.draw_networkx(G)\n\n[(1, 2, 2), (2, 5, 1), (3, 4, 1)]\n\n\n\n\n\n\n\n\n\n\nnx.degree_pearson_correlation_coefficient?\n\n\nSignature:\nnx.degree_pearson_correlation_coefficient(\n    G,\n    x='out',\n    y='in',\n    weight=None,\n    nodes=None,\n)\nDocstring:\nCompute degree assortativity of graph.\nAssortativity measures the similarity of connections\nin the graph with respect to the node degree.\nThis is the same as degree_assortativity_coefficient but uses the\npotentially faster scipy.stats.pearsonr function.\nParameters\n----------\nG : NetworkX graph\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\nnodes: list or iterable (optional)\n    Compute pearson correlation of degrees only for specified nodes.\n    The default is all nodes.\nReturns\n-------\nr : float\n   Assortativity of graph by degree.\nExamples\n--------\n&gt;&gt;&gt; G = nx.path_graph(4)\n&gt;&gt;&gt; r = nx.degree_pearson_correlation_coefficient(G)\n&gt;&gt;&gt; print(f\"{r:3.1f}\")\n-0.5\nNotes\n-----\nThis calls scipy.stats.pearsonr.\nReferences\n----------\n.. [1] M. E. J. Newman, Mixing patterns in networks\n       Physical Review E, 67 026126, 2003\n.. [2] Foster, J.G., Foster, D.V., Grassberger, P. & Paczuski, M.\n   Edge direction and the structure of networks, PNAS 107, 10815-20 (2010).\nFile:      ~/anaconda3/envs/py38/lib/python3.8/site-packages/networkx/algorithms/assortativity/correlation.py\nType:      function\n\n\n\n\nnx.draw_networkx\n\n\n결과값 해석.. 어케 함..\n\n\n\n커뮤니티 자원 할당\n\\[Community Common Neighbor(u,v) = \\sum_{w in N(v) \\cap N(u)} \\dfrac{f(w)}{|N(w)|}\\]\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\npreds = nx.ra_index_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0), (3, 4, 0)]"
  },
  {
    "objectID": "posts/graph5-1.html#임베딩-기반-방법",
    "href": "posts/graph5-1.html#임베딩-기반-방법",
    "title": "CH5. 그래프에서의 머신러닝 문제(링크예측)",
    "section": "임베딩 기반 방법",
    "text": "임베딩 기반 방법\n\n주어진 그래프에 대한 각 노드 쌍을 특징 벡터(x)로 표현\n클래스 라벨(y)를 해당 노드 쌍 각각에 할당\n\n- 전체 프로세스\n1 그래프 G의 각 노드에 대해 해당 임베딩 벡터를 node2vec 알고리즘 사용해 계산\n2 그래프 가능한 모든 노드 쌍에 대해 edge2vec알고리즘 사용해 임베딩 계산\n- 데이터셋: cora\n\n인용 데이터셋을 사용해 다음과 같이 networkx 그래프 작성\n\n\nimport networkx as nx\nimport pandas as pd\n\nedgelist = pd.read_csv(\"cora.cites\", sep='\\t', header=None, names=[\"target\", \"source\"])\nG = nx.from_pandas_edgelist(edgelist)\ndraw_graph(G)\n\n\n\n\n\n\n\n\n\n그래프 G에서 훈련 및 테스트 데이터셋 생성\n\n\n훈련 및 테스트 데이터셋에서 그래프 G의 실제 노드 쌍을 나타내는 집합+ G 실제 노드를 나타내지 않는 노드 쌍도 포함\n양의 인스턴스(클래스 라벨1) : 실제 간선 나타내느 ㄴ쌍\n음의 인스턴스(클래스 라벨0) : 실제 간선을 나타내지 않는 쌍\n\n\nfrom stellargraph.data import EdgeSplitter\n\nedgeSplitter = EdgeSplitter(G)\ngraph_test, samples_test, labels_test = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n2023-04-06 22:36:33.732989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n** Sampled 527 positive and 527 negative edges. **\n\n\n\ngraph_test 모든 노드 포함, 간선의 부분 집합만 포함하는 원본 그래프의 부분 집합\nsample_test 각각의 노드 쌍 포함, 실제 간선을 나태나는 노드쌍과 실제 모서리를 나타내지 않는 노드 쌍이 포함\nlabel_test sample_test와 같은 길이의 벡터. 0(샘플테스트 백터에서 음의인스터스를 나타내는 위치)또는 1(양의 인스턴스 위치)만 포함\n\n\n훈련 세트 생성\n\n\nedgeSplitter = EdgeSplitter(graph_test, G)\ngraph_train, samples_train, labels_train = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n** Sampled 475 positive and 475 negative edges. **\n\n\n\n훈련 세트 특징 벡터 생성\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder\n\nnode2vec = Node2Vec(graph_train)  # 각 노드에 대한 임베딩 생성\nmodel = node2vec.fit()\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) #훈련세트에 포함된 각 노드 쌍의 임베딩 생성 -&gt; 모델 학습 위한 특징 벡터로 사용\ntrain_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.05it/s]\n\n\n\n테스트 세트 특징 벡터 생성\n\n\ntest_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]\n\n\ntrain_embedding특징 공간과 train_labels 라벨 할당을 이용해 머신러닝 알고리즘 학습\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000)\nrf.fit(train_embeddings, labels_train);\n\n\nfrom sklearn import metrics\n\ny_pred = rf.predict(test_embeddings)\n\nprint('Precision:', metrics.precision_score(labels_test, y_pred))\nprint('Recall:', metrics.recall_score(labels_test, y_pred))\nprint('F1-Score:', metrics.f1_score(labels_test, y_pred))\n\nPrecision: 0.8952380952380953\nRecall: 0.713472485768501\nF1-Score: 0.7940865892291447"
  },
  {
    "objectID": "posts/graph3-1.html",
    "href": "posts/graph3-1.html",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\n비지도 알고리즘은 스스로 클러스터를 찾고, 패턴 발견, 이상치 감지"
  },
  {
    "objectID": "posts/graph3-1.html#그래프-분해graph-factorization",
    "href": "posts/graph3-1.html#그래프-분해graph-factorization",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "그래프 분해(Graph Factorization)",
    "text": "그래프 분해(Graph Factorization)\n\n그래프의 인접 행렬을 분해하여 노드 및 엣지에 대한 임베딩을 생성\n행렬 분해 알고리즘을 사용하며, 높은 차원의 임베딩 생성 가능\n단점으로는 계산 복잡도가 높고, 대규모 그래프에서는 적용이 어려울 수 있음\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\ndraw_graph(G)\n\n\n\n\n\n\n\n\n\nfrom pathlib import Path\nPath(\"gem/intermediate\").mkdir(parents=True, exist_ok=True)\n\n\nfrom gem.embedding.gf import GraphFactorization\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngf = GraphFactorization(d=2,  data_set=None,max_iter=10000, eta=1*10**-4, regu=1.0)  #2차원 임베딩 공간 생성\ngf.learn_embedding(G)\n\n\n\n\n\n\n\n\n./gf not found. Reverting to Python implementation. Please compile gf, place node2vec in the path and grant executable permission\n\n\narray([[-0.00023347,  0.00423037],\n       [-0.00023539,  0.00422975],\n       [-0.00022958,  0.00424174],\n       [-0.00024031,  0.0042126 ],\n       [-0.00021981,  0.00425632],\n       [-0.0002757 ,  0.00433198],\n       [-0.00043729,  0.00388277],\n       [-0.00120009,  0.00441975],\n       [ 0.00012455,  0.00342726],\n       [ 0.00703928,  0.00360693],\n       [ 0.00545482, -0.00073236],\n       [-0.00191172, -0.00167922],\n       [-0.00762759, -0.00011384],\n       [-0.001844  ,  0.00466985],\n       [ 0.00523278,  0.01142738],\n       [ 0.0052346 ,  0.01142813],\n       [ 0.00523662,  0.0114319 ],\n       [ 0.00523082,  0.01144355],\n       [ 0.00519996,  0.01138897],\n       [ 0.00519592,  0.01136481],\n       [ 0.00511132,  0.01157818],\n       [ 0.0049727 ,  0.01154795],\n       [ 0.00689695,  0.01202013],\n       [ 0.00920799,  0.0153358 ]])\n\n\n\nmax_iter: 최대 반복 횟수\neta: 학습률\nregu: 정규화 계수\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = gf.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/graph3-1.html#고차-근접-보존-임베딩hope",
    "href": "posts/graph3-1.html#고차-근접-보존-임베딩hope",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "고차 근접 보존 임베딩(HOPE)",
    "text": "고차 근접 보존 임베딩(HOPE)\n\n고차 근접성 유지, 임베딩 대칭 속성 강제X\n그래프의 2차원 행렬을 생성하여 노드 및 엣지에 대한 임베딩을 생성\n그래프의 고차원 구조를 보존하기 위해 그래프의 라플라시안 행렬을 이용하여 행렬 생성\n계산이 간단하고, 다양한 유형의 그래프에 적용 가능\n\n\nimport networkx as nx\nfrom gem.embedding.hope import HOPE\n\nG = nx.barbell_graph(m1=10, m2=4)  #바벨 그래프 생성\ndraw_graph(G)\n\nhp = HOPE(d=4, beta=0.01)\nhp.learn_embedding(G)\n\n\n\n\n\n\n\n\nSVD error (low rank): 0.052092\n\n\narray([[-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07024409, -0.07024348, -0.07024409, -0.07024348],\n       [-0.07104037, -0.07104201, -0.07104037, -0.07104201],\n       [-0.00797181, -0.00799433, -0.00797181, -0.00799433],\n       [-0.00079628, -0.00099787, -0.00079628, -0.00099787],\n       [ 0.00079628, -0.00099787,  0.00079628, -0.00099787],\n       [ 0.00797181, -0.00799433,  0.00797181, -0.00799433],\n       [ 0.07104037, -0.07104201,  0.07104037, -0.07104201],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348],\n       [ 0.07024409, -0.07024348,  0.07024409, -0.07024348]])\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = hp.get_embedding()[x,2:]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=20)\n\n\n\n\n\n\n\n\n\n방향이 없으므로 원천 노드와 대상 노드 간에 차이가 없다."
  },
  {
    "objectID": "posts/graph3-1.html#전역-구조-정보를-통한-그래프-표현graphrep",
    "href": "posts/graph3-1.html#전역-구조-정보를-통한-그래프-표현graphrep",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "전역 구조 정보를 통한 그래프 표현(GraphRep)",
    "text": "전역 구조 정보를 통한 그래프 표현(GraphRep)\n\n그래프를 분해하여 노드 및 엣지의 임베딩을 생성\n그래프의 구조 정보와 노드의 속성 정보를 동시에 고려하여 임베딩 생성\n다양한 유형의 그래프에 적용 가능하며, 다른 방법들에 비해 높은 임베딩 품질을 보장\n\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.grarep import GraRep\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngr = GraRep(dimensions=2,order=3)  #dimension:임베딩 공간 차수, order:노드사이의 최대 근접 차수\ngr.fit(G)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nida = 4 #4번째와 5번째\nidb = 5\nfor x in G.nodes():\n    \n    v = gr.get_embedding()[x]\n    ax.scatter(v[ida],v[idb], s=1000)\n    ax.annotate(str(x), (v[ida],v[idb]), fontsize=12)"
  },
  {
    "objectID": "posts/graph3-1.html#deepwalk",
    "href": "posts/graph3-1.html#deepwalk",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "DeepWalk",
    "text": "DeepWalk\n\n노드 간의 구조적 유사성을 학습하여 노드를 저차원 벡터로 표현\n노드가 깊이 우선 탐색(DFS) 방식으로 샘플링된 문맥을 윈도우로 사용해 노드의 임베딩 학습\n무방향성 그래프에서 랜덤 워크를 수행(무작위 경로 생성)하여, 노드의 이웃 노드를 샘플링하고 이를 바탕으로 임베딩 생성\n이웃 노드를 샘플링할 때, 유사한 패턴의 노드를 더 많이 샘플링하여 군집 구조 정보를 잘 반영\n계산이 간단하고, 다양한 유형의 그래프에 적용 가능\n\n- 그래프 생성 알고리즘\n1 랜덤 워크 생성: 입력 그래프 \\(G\\)의 각 노드에 대해 고정된 최대 길이(\\(t\\))를 갖는 랜덤 워크 세트 계산\n2 skip-gram학습\n3 임베딩 생성\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ndw = DeepWalk(dimensions=2)\ndw.fit(G)\n\n\n\n\n\n\n\n\n\nDeepWalkr가 영역 1을 영역 3과 분리\n영역 2에 속하는 노드에 의해서 오염됬고 임베딩 공간에서 명확한 구분이 보이지 않는다.\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = dw.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/graph3-1.html#node2vec",
    "href": "posts/graph3-1.html#node2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Node2Vec",
    "text": "Node2Vec\n\n랜덤워크를 그래프에 편향된 무작위경로 생성\n랜덤 워크를 수행하여 노드에 대한 임베딩을 생성\n노드 간의 구조적 유사성과 동시에 노드의 속성 정보를 고려하여 임베딩을 생성\n랜덤 워크 수행 시, 노드의 탐색 패턴을 조절하는 파라미터를 추가하여 다양한 유형의 그래프에서 임베딩 품질을 조정할 수 있음\n다양한 그래프 분석 및 예측에 적용 가능\n너비 우선 탐색(BFS), 깊이 우선 탐색(DFS)을 병합해 그래프 탐색\np 랜덤 워크가 이전 노드로 돌아갈 확률\nq 랜덤 워크가 새로운 노드로 통과할 확률\n\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\nnode2vec = Node2Vec(G, dimensions=2)\nmodel = node2vec.fit(window=10)\nembeddings = model.wv\n\n\n\n\n\n\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:00&lt;00:00, 231.58it/s]\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = model.wv[str(x)]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDeepWalk에 비해 임베딩 공간에서 노드 간 더 나은 분리도"
  },
  {
    "objectID": "posts/graph3-1.html#edge2vec",
    "href": "posts/graph3-1.html#edge2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Edge2Vec",
    "text": "Edge2Vec\n\n랜덤 워크를 수행하여 엣지에 대한 임베딩을 생성\n노드-엣지-노드 패턴을 이용하여 엣지의 구조 정보를 고려하여 임베딩 생성\n노드 및 엣지 분석에 적용 가능\n\\(v_i, v_j\\) : 인접한 노드\n\\(f(v_i), f(v_j)\\) : Node2Vec으로 계산된 임베딩\n\n\n\n\n연산자\n방정식\n클래스이름\n\n\n\n\n평균\n\\(\\dfrac{f(v_i)+f(v_j)}{2}\\)\nAverageEmbedder\n\n\n아다마르\n\\(f(v_i)*f(v_j)\\)\nHamamarEmbedder\n\n\nL1가중\n\\(|f(v_i)-f(v_j)|\\)\nWeightedL1Embedder\n\n\nL2가중\n\\(|f(v_i)-f(v_j)|^2\\)\nWeightedL2Embedder\n\n\n\n- HadamardEmbedder\n\nfrom node2vec.edges import HadamardEmbedder\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n- WeightedL1Embedder\n\nfrom node2vec.edges import WeightedL1Embedder\n\nedges_embs2 = WeightedL1Embedder(keyed_vectors=model.wv)\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs2[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\n- WeightedL2Embedder\n\nfrom node2vec.edges import WeightedL2Embedder\n\nedges_embs3 = WeightedL2Embedder(keyed_vectors=model.wv)\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs3[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/graph3-1.html#graph2vec",
    "href": "posts/graph3-1.html#graph2vec",
    "title": "CH3. 비지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Graph2Vec",
    "text": "Graph2Vec\n\n주어진 그래프 세트에서 각 점이 그래프를 나타내는 임베딩 공간 생성\n그래프 자체를 벡터화하여 그래프에 대한 임베딩을 생성\n전체그래프의 구조 정보와 그래프 내 노드 및 엣지의 속성 정보를 고려하여 임베딩 생성\n그래프 분류 및 유사도 측정에 적용 가능\n\n\nimport random\nimport matplotlib.pyplot as plt\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)\n    k = random.randint(5, n)\n    p = random.uniform(0, 1)\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]   #20개의 watts_strogatz 그래프 생성\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10) #2차원으로 초기화 \n\n# 학습\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/graph8(logistic, amt+time).html",
    "href": "posts/graph8(logistic, amt+time).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time-f1:0.009370)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean})\n\n\n\n\n\n\n\n\ncity_pop\namt\ntime\n\n\nis_fraud\n\n\n\n\n\n\n\n0\n83870.443845\n67.743047\n12.813152\n\n\n1\n96323.951715\n530.573492\n13.915917\n\n\n\n\n\n\n\n\n_df2.groupby(by=['category']).agg({'is_fraud':np.mean})\n\n\n\n\n\n\n\n\nis_fraud\n\n\ncategory\n\n\n\n\n\nentertainment\n0.003907\n\n\nfood_dining\n0.002628\n\n\ngas_transport\n0.007570\n\n\ngrocery_net\n0.004802\n\n\ngrocery_pos\n0.022539\n\n\nhealth_fitness\n0.002408\n\n\nhome\n0.002488\n\n\nkids_pets\n0.003440\n\n\nmisc_net\n0.023023\n\n\nmisc_pos\n0.004859\n\n\npersonal_care\n0.003774\n\n\nshopping_net\n0.027628\n\n\nshopping_pos\n0.011342\n\n\ntravel\n0.004886\n\n\n\n\n\n\n\n\n_df2.groupby(by=['time']).agg({'is_fraud':np.mean}).plot()\n\n\n\n\n\n\n\n\n\n그래프상 시간을 3등분 하거나 2등분 해서 적합시키면 좋을 거 같다.\n\n3등분: 20 ~ 04, 04 ~ 12, 12 ~ 20\n\n\n2등분: 06 ~ 18, 18 ~ 06\n\n사기거래와 사기거래가 아닌 그룹에서 데이터 범주가 차이가 나는걸 보면\n금액, 시간..\n\n\n_df3=_df2[['amt','time','category','is_fraud']]\n_df3\n\n\n\n\n\n\n\n\namt\ntime\ncategory\nis_fraud\n\n\n\n\n3\n45.00\n0\ngas_transport\n0\n\n\n5\n94.63\n0\ngas_transport\n0\n\n\n6\n44.54\n0\ngrocery_net\n0\n\n\n7\n71.65\n0\ngas_transport\n0\n\n\n8\n4.27\n0\nmisc_pos\n0\n\n\n...\n...\n...\n...\n...\n\n\n1048567\n39.96\n16\nkids_pets\n0\n\n\n1048568\n20.67\n16\nentertainment\n0\n\n\n1048569\n6.03\n16\nfood_dining\n0\n\n\n1048571\n116.94\n16\nmisc_pos\n0\n\n\n1048574\n6.81\n16\nmisc_pos\n0\n\n\n\n\n651430 rows × 4 columns\n\n\n\n\n_df3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 4 columns):\n #   Column    Non-Null Count   Dtype  \n---  ------    --------------   -----  \n 0   amt       651430 non-null  float64\n 1   time      651430 non-null  int64  \n 2   category  651430 non-null  object \n 3   is_fraud  651430 non-null  int64  \ndtypes: float64(1), int64(2), object(1)\nmemory usage: 24.9+ MB\n\n\n\n_df4=_df2[['amt','time','is_fraud']]\n_df4\n\n\n\n\n\n\n\n\namt\ntime\nis_fraud\n\n\n\n\n3\n45.00\n0\n0\n\n\n5\n94.63\n0\n0\n\n\n6\n44.54\n0\n0\n\n\n7\n71.65\n0\n0\n\n\n8\n4.27\n0\n0\n\n\n...\n...\n...\n...\n\n\n1048567\n39.96\n16\n0\n\n\n1048568\n20.67\n16\n0\n\n\n1048569\n6.03\n16\n0\n\n\n1048571\n116.94\n16\n0\n\n\n1048574\n6.81\n16\n0\n\n\n\n\n651430 rows × 3 columns\n\n\n\n\ndata=np.hstack([_df4.values[:,:]])\n\n\ndata\n\narray([[ 45.  ,   0.  ,   0.  ],\n       [ 94.63,   0.  ,   0.  ],\n       [ 44.54,   0.  ,   0.  ],\n       ...,\n       [  6.03,  16.  ,   0.  ],\n       [116.94,  16.  ,   0.  ],\n       [  6.81,  16.  ,   0.  ]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {:.6f}\".format(acc))\nprint(\"Precision: {:.6f}\".format(precision))\nprint(\"Recall: {:.6f}\".format(recall))\nprint(\"F1 score: {:.6f}\".format(f1score))\n\nAccuracy: 0.051349\nPrecision: 0.005162\nRecall: 0.051349\nF1 score: 0.009370\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.051348571604009643\nPrecision:0.004232713983377072\nRecall: 0.04223025149824091\nF1 score: 0.007685871263604649\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='micro')\nrecall = recall_score(y_test, y_pred, average='micro')\nf1score = f1_score(y_test, y_pred, average='micro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.051348571604009643\nPrecision:0.051348571604009643\nRecall: 0.051348571604009643\nF1 score: 0.051348571604009643\n\n\n- average 매개 변수\n\nNone: 클래스별 metric 값을 계산\nmicro: 모든 샘플을 하나의 그룹으로 취급하여 metric 값을 계산\nmacro: 클래스별 metric 값을 동일한 가중치로 더하여 산술평균을 계산\nweighted: 클래스별 metric 값을 라벨 수로 가중 평균하여 계산\nsamples: 샘플마다 metric 값을 계산"
  },
  {
    "objectID": "posts/graph3-3.html",
    "href": "posts/graph3-3.html",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph3-3.html#unsupervised-graph-representation-learning-using-graph-convnet",
    "href": "posts/graph3-3.html#unsupervised-graph-representation-learning-using-graph-convnet",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "Unsupervised graph representation learning using Graph ConvNet",
    "text": "Unsupervised graph representation learning using Graph ConvNet\n\n스펙트럼 그래프 합성곱\n\n- 키프와 웰링이 제안한 정규화\n\\[H^t=\\sigma(\\hat D^{-\\dfrac{1}{2}} \\hat A \\hat D^{-\\dfrac{1}{2}}XW)\\]\n\n\\(\\hat D\\)는 \\(\\hat A\\)의 대각 노드 차수 행렬\n\n\n#from networkx import karate_club_graph, to_numpy_matrix\nimport numpy as np\nimport networkx as nx\nfrom scipy.linalg import sqrtm\nimport matplotlib.pyplot as plt\n\nG = nx.barbell_graph(m1=10, m2=4)\n\norder = np.arange(G.number_of_nodes())\nA = nx.to_numpy_matrix(G, nodelist=order)\nI = np.eye(G.number_of_nodes())\n\n- 자체 루프 추가, 대각 노드 차수 행렬 준비\n\nnp.random.seed(7)\n\nA_hat = A + np.eye(G.number_of_nodes()) # add self-connections  # G의 노드 수만큼 단위 행렬 만들기\n\nD_hat = np.array(np.sum(A_hat, axis=0))[0]\nD_hat = np.array(np.diag(D_hat))\nD_hat = np.linalg.inv(sqrtm(D_hat))\n# D_hat은 A_hat의 각 열의 합으로 이루어진 대각행렬의 역행렬\n# 그래프 각 노드의 연결 강도를 정규화\n\nA_hat = D_hat @ A_hat @ D_hat\n#A_hat을 정규화된 그래프 연결 행렬로 \n\n- 2개의 레이어로 구성된 GCN(그래프합성곱신경망)만들기\n\n가중치 \\(W\\)\n\n\ndef glorot_init(nin, nout):  # 각  GCN의 가중치 행렬 난수로 초기화\n  sd = np.sqrt(6.0 / (nin + nout))\n  return np.random.uniform(-sd, sd, size=(nin, nout))\n\nclass GCNLayer():\n  def __init__(self, n_inputs, n_outputs):\n      self.n_inputs = n_inputs\n      self.n_outputs = n_outputs\n      self.W = glorot_init(self.n_outputs, self.n_inputs)\n      self.activation = np.tanh\n      \n  def forward(self, A, X): #인접행렬과 피처행렬을 이용해 그래프 신경망을 순방향으로 계산. 결과 반환\n      self._X = (A @ X).T # (N,N)*(N,n_outputs) ==&gt; (n_outputs,N)\n      H = self.W @ self._X # (N, D)*(D, n_outputs) =&gt; (N, n_outputs)\n      H = self.activation(H)\n      return H.T # (n_outputs, N)\n\n- 네트워크 만들고 순방향 패스 계산. 네트워크를 통해 신호 전파\n\n\ngcn1 = GCNLayer(G.number_of_nodes(), 8)\ngcn2 = GCNLayer(8, 4)\ngcn3 = GCNLayer(4, 2)\n\nH1 = gcn1.forward(A_hat, I)   #중간 출력 \nH2 = gcn2.forward(A_hat, H1)  #중간 출력\nH3 = gcn3.forward(A_hat, H2)  #최종 결과\n\nembeddings = H3  # 최종 결과\n\n\nembeddings\n\nmatrix([[-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.01476752, -0.05053743],\n        [-0.03171266, -0.05642689],\n        [-0.08356075, -0.03146726],\n        [-0.11696591, -0.00297877],\n        [-0.10436451,  0.0226648 ],\n        [-0.06008126,  0.01901998],\n        [-0.00104536, -0.05409599],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606],\n        [ 0.01041858, -0.06209606]])\n\n\n\nfrom gem.embedding.gf import GraphFactorization\n\n\ndef draw_graph(G, filename=None, node_size=50):\n  pos_nodes = nx.spring_layout(G)\n  nx.draw_networkx(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray')\n  \n  pos_attrs = {}\n  for node, coords in pos_nodes.items():\n    pos_attrs[node] = (coords[0], coords[1] + 0.08)\n\n  plt.axis('off')\n  axis = plt.gca()\n  axis.set_xlim([1.2*x for x in axis.get_xlim()])\n  axis.set_ylim([1.2*y for y in axis.get_ylim()])\n\nembeddings = np.array(embeddings)\ndraw_graph(G)\n\n\n\n\n\n\n\n\n\nplt.scatter(embeddings[:, 0], embeddings[:, 1])\nplt.savefig('embedding_gcn.png',dpi=300)"
  },
  {
    "objectID": "posts/graph3-3.html#unsupervised-gcn-training-using-similarity-graph-distance",
    "href": "posts/graph3-3.html#unsupervised-gcn-training-using-similarity-graph-distance",
    "title": "CH3. 비지도 그래프 학습(그래프신경망)",
    "section": "Unsupervised GCN training using similarity graph distance",
    "text": "Unsupervised GCN training using similarity graph distance\n\nStellarGraph 사용\n대상 변수 없이 비지도 방식으로 벡터 삽입하는 방법\n\n\n!pip install -q stellargraph[demos]==1.2.1\n\n\npip install chardet\n\nCollecting chardet\n  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 10.0 MB/s eta 0:00:00\nInstalling collected packages: chardet\nSuccessfully installed chardet-5.1.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport os\n\nimport stellargraph as sg\nfrom stellargraph.mapper import FullBatchNodeGenerator\nfrom stellargraph.layer import GCN\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics, Model\nfrom sklearn import preprocessing, model_selection\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n- 데이터셋: PROTENIS\n\ndataset = sg.datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\nEach graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.\n\n\n\n39개 노드, 73개 간선이 있는 1,114개 그래프로 구성\n각 노드는 4개의 속성으로 설명되며 두 클래스 중 하나에 속한다.\n\n\n# let's print some info to better understand the dataset\nprint(graphs[0].info())\ngraph_labels.value_counts().to_frame()\n\nStellarGraph: Undirected multigraph\n Nodes: 42, Edges: 162\n\n Node types:\n  default: [42]\n    Features: float32 vector, length 4\n    Edge types: default-default-&gt;default\n\n Edge types:\n    default-default-&gt;default: [162]\n        Weights: all 1 (default)\n        Features: none\n\n\n\n\n\n\n\n\n\nlabel\n\n\n\n\n1\n663\n\n\n2\n450\n\n\n\n\n\n\n\n- 모델 만들기\n\n# TODO\ngenerator = sg.mapper.PaddedGraphGenerator(graphs)\n\n\n# 64,32 사이즈의 레이어 2개를 포함한 GCN model 정의\n# ReLU 활성화 함수는 레이어 간 비선형을 추가하고자 사용\n\ngc_model = sg.layer.GCNSupervisedGraphClassification(\n    [64, 32], [\"relu\", \"relu\"], generator, pool_all_layers=True\n)\n\n\n# 다음 레이어에 연결할 수 있도록 GC 레이어의 입력 및 출력 텐서 확인\n\n\ninp1, out1 = gc_model.in_out_tensors()\ninp2, out2 = gc_model.in_out_tensors()\n\nvec_distance = tf.norm(out1 - out2, axis=1)\n\n\n# 모델 생성, 임베딩을 찾기 쉽게 반사 모델 생성\npair_model = Model(inp1 + inp2, vec_distance)\nembedding_model = Model(inp1, out1)\n\n- 훈련\n\n입력 그래프의 각 쌍에 유사성 점수 할당\n단순화를 위해 그래프의 라플라시안 스펙트럼 사이의 거리 사용\n\n\ndef graph_distance(graph1, graph2):\n    spec1 = nx.laplacian_spectrum(graph1.to_networkx(feature_attr=None))\n    spec2 = nx.laplacian_spectrum(graph2.to_networkx(feature_attr=None))\n    k = min(len(spec1), len(spec2))\n    return np.linalg.norm(spec1[:k] - spec2[:k])\n\n\ngraph_idx = np.random.RandomState(0).randint(len(graphs), size=(100, 2))\ntargets = [graph_distance(graphs[left], graphs[right]) for left, right in graph_idx]\ntrain_gen = generator.flow(graph_idx, batch_size=10, targets=targets)\n\n&lt;class 'networkx.utils.decorators.argmap'&gt; compilation 20:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n\n\n- 모델 학습\n\npair_model.compile(optimizers.Adam(1e-2), loss=\"mse\")\n\n\nhistory = pair_model.fit(train_gen, epochs=500, verbose=0)\nsg.utils.plot_history(history)\n\n\n\n\n\n\n\n\n- 시각화\n\n# 임베딩 검색\n\nembeddings = embedding_model.predict(generator.flow(graphs))\n\n1113/1113 [==============================] - 1s 531us/step\n\n\n\n# 차원 축소를 위해 TSNE 사용\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(2)\ntwo_d = tsne.fit_transform(embeddings)\n\n\n출력이 32차원이므로 임베딩을 2차원 공간에 플로팅해 임베딩 정상적 평가\n\n\nplt.scatter(two_d[:, 0], two_d[:, 1], c=graph_labels.cat.codes, cmap=\"jet\", alpha=0.4)\nplt.savefig('embedding_TSNE.png',dpi=300)\n\n\n\n\n\n\n\n\n\n빨간색=1, 파란색=0"
  },
  {
    "objectID": "posts/graph8(logistic+graph)_.html",
    "href": "posts/graph8(logistic+graph)_.html",
    "title": "CH8. 신용카드 거래 분석(로지스틱+그래프)",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndf\n\nimport pandas as pd\ndf = pd.read_csv(\"fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ndf[\"is_fraud\"].value_counts()\n\n0    208514\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  2.53it/s]\n\n\n\n\n\n\n\n_df2\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\nNameError: name '_df' is not defined\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n\ndf = _df2 \n\n\ndef build_graph_bipartite2(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled2 = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled2 = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled2.is_fraud.value_counts())\nG_down2 = build_graph_bipartite(df_downsampled2)\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges2, test_edges2, train_labels2, test_labels2 = train_test_split(list(range(len(G_down2.edges))), \n                                                                      list(nx.get_edge_attributes(G_down2, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs2 = list(G_down2.edges)\ntrain_graph2 = G_down2.edge_subgraph([edgs[x] for x in train_edges2]).copy()\ntrain_graph2.add_nodes_from(list(set(G_down2.nodes) - set(train_graph2.nodes)))\n\n\nnode2vec_train2 = Node2Vec(train_graph2, weight_key='weight')\nmodel_train2 = node2vec_train2.fit(window=10)\n\n\n\ntraing(graph), test(logistic)\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges2]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred2 = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred2)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred2)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred2)) \n\nNameError: name 'HadamardEmbedder' is not defined"
  },
  {
    "objectID": "posts/graph8-1.html",
    "href": "posts/graph8-1.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8-1.html#네트워크-토폴로지",
    "href": "posts/graph8-1.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\ndegree(G)\n\nNameError: name 'degree' is not defined\n\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.17,  48.31,  76.29, 146.7 ])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.10159189882353903\n-0.8017506210033467\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8-1.html#커뮤니티-감지",
    "href": "posts/graph8-1.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n128928     0\n60346      0\n88225      1\n13366      1\n46256      2\n          ..\n6269      65\n59264     68\n33729      7\n176829    70\n77640     79\nLength: 216156, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n12    4019\n71    3999\n27    3743\n52    3739\n43    3679\n      ... \n32    1110\n93    1097\n49    1060\n26    1003\n33     892\nLength: 96, dtype: int64\n\n\n\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n2500부근에 형성되었고 ..\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n48    8.684864\n13    6.956522\n55    6.781235\n45    6.743257\n88    6.338616\n        ...   \n93    0.996377\n75    0.952381\n51    0.765957\n82    0.737265\n33    0.335946\nLength: 96, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 6\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8-1.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8-1.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.6953125\nRecall: 0.156140350877193\nF1-Score: 0.2550143266475645\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6813353566009105\nRecall: 0.787719298245614\nF1-Score: 0.7306753458096015\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5925925925925926\nRecall: 0.028070175438596492\nF1-Score: 0.05360134003350084\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.5833333333333334\nRecall: 0.02456140350877193\nF1-Score: 0.04713804713804714\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.25it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.0429862559854\nHomogeneity: 0.03813140300201337\nCompleteness: 0.049433212382250756\nV-Measure: 0.0430529554606017\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.09395128496638593\nHomogeneity: 0.08960753766432715\nCompleteness: 0.09886731281849871\nV-Measure: 0.09400995872350308\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17593048106009063\nHomogeneity: 0.17598531397290276\nCompleteness: 0.17597737533152563\nV-Measure: 0.17598134456268477\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.1362053730791375\nHomogeneity: 0.1349991253997398\nCompleteness: 0.1375429939044335\nV-Measure: 0.13625918760275774\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자."
  },
  {
    "objectID": "posts/graph4-1.html",
    "href": "posts/graph4-1.html",
    "title": "CH4. 지도 그래프 학습(특징기반방법)",
    "section": "",
    "text": "ref\n\n그래프 머신러닝\ngithub\n\n\n\n특징기반방법(Feature based methods)\n\n설명적인 특징 집합을 특정 출력에 매핑하는 함수 찾기.\n해당 개념을 학습할 만큼 전체를 충분히 대표하도록 주의 깊게 설계.\n평균 차수, 전체 효율성, 특징정인 경로 길이에 의존\n\n\nsellargraph 통해서 데이터셋 로드\n\n\nfrom stellargraph import datasets\nfrom IPython.display import display, HTML\n\ndataset = datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\n2023-04-06 21:04:18.948281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nEach graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.\n\n\n\nstellargraph형식에서 networkx 형식으로 그래프 변환\n\n\nstellargraph 표현에서 numpy 인접행렬로 그래프 변환\n인접행렬을 사용해 networkx 표현으로 돌리기\n\n\n# tellargraph 형태에서 numpy인접행렬로 변환\nadjs = [graph.to_adjacency_matrix().A for graph in graphs]\n\n# Pandas.Series로 구성된 라벨을 numpy array로 변환\nlabels = graph_labels.to_numpy(dtype=int)\n\n\n각 그래프에 대해 설명하기 위해 전역 측정 지표 계산\n\n\n간선수, 평균 클러스터 계수, 전역 효율성 선택\n\n\nimport numpy as np\nimport networkx as nx\n\nmetrics = []\nfor adj in adjs:\n  G = nx.from_numpy_matrix(adj)\n\n  # 기본 속성\n  num_edges = G.number_of_edges()\n\n  # 클러스터링 방법\n  cc = nx.average_clustering(G)\n\n  # 효율성 측정\n  eff = nx.global_efficiency(G)\n\n  metrics.append([num_edges, cc, eff])\n\n\nscikit-learn 유틸리티를 활용해 훈련 및 테스트 세트를 생성\n\n데이터셋의 70% 훈련, 30% 테스트\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)\n\n\n머신러닝 알고리즘 학습 시작\n\nscikit-learn의 SVC모듈 사용\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclf = svm.SVC()\nclf.fit(X_train_scaled, y_train)\n\ny_pred = clf.predict(X_test_scaled)\n\nprint('Accuracy', accuracy_score(y_test,y_pred))\nprint('Precision', precision_score(y_test,y_pred))\nprint('Recall', recall_score(y_test,y_pred))\nprint('F1-score', f1_score(y_test,y_pred))\n\nAccuracy 0.7455089820359282\nPrecision 0.7709251101321586\nRecall 0.8413461538461539\nF1-score 0.8045977011494253"
  },
  {
    "objectID": "posts/graph8(over-sampling).html",
    "href": "posts/graph8(over-sampling).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(over-sampling)",
    "section": "",
    "text": "import pandas as pd\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nimport pandas as pd\ndf = pd.read_csv(\"fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ndf[\"is_fraud\"].value_counts()\n\n0    208514\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\n# 기존 코드 (down)\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\nfrom sklearn.utils import resample\n\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_min_oversampled = resample(df_minority,\n                              n_samples=len(df_majority),\n                              replace=True,\n                              random_state=42)\n\ndf_oversampled = pd.concat([df_majority, df_min_oversampled])\n\nprint(df_oversampled.is_fraud.value_counts())\nG_over = build_graph_bipartite(df_oversampled)\n\n0    208514\n1    208514\nName: is_fraud, dtype: int64\n\n\n\noversampling했다\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_over.edges))), \n                                                                      list(nx.get_edge_attributes(G_over, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_over.edges)\ntrain_graph = G_over.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_over.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:15&lt;00:00,  1.53s/it]\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.0\nRecall: 0.0\nF1-Score: 0.0\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.0\nRecall: 0.0\nF1-Score: 0.0\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.0\nRecall: 0.0\nF1-Score: 0.0\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\nnod2vec_unsup = Node2Vec(G_over, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:17&lt;00:00,  1.79s/it]\n\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_over, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_over.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.00544668790993928\nHomogeneity: 0.013903066513115778\nCompleteness: 0.0033926306209409642\nV-Measure: 0.005454301010451627\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.022759004483245575\nHomogeneity: 0.06217623778028962\nCompleteness: 0.013933972719173142\nV-Measure: 0.022765986201483266\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.04571547270228404\nHomogeneity: 0.1238534042248765\nCompleteness: 0.02803615788549803\nV-Measure: 0.04572234651623731\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.04580696581262077\nHomogeneity: 0.12668781634894563\nCompleteness: 0.027962918433326666\nV-Measure: 0.045813698590560094"
  },
  {
    "objectID": "posts/graph8(undersampling cluster).html",
    "href": "posts/graph8(undersampling cluster).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(undersampling-cluster)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(undersampling cluster).html#네트워크-토폴로지",
    "href": "posts/graph8(undersampling cluster).html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(undersampling-cluster)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8(undersampling cluster).html#커뮤니티-감지",
    "href": "posts/graph8(undersampling cluster).html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(undersampling-cluster)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8(undersampling cluster).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(undersampling cluster).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(undersampling-cluster)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.47it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7349397590361446\nRecall: 0.15996503496503497\nF1-Score: 0.26274228284278534\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6856264411990777\nRecall: 0.7797202797202797\nF1-Score: 0.7296523517382413\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5737704918032787\nRecall: 0.030594405594405596\nF1-Score: 0.05809128630705394\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.609375\nRecall: 0.03409090909090909\nF1-Score: 0.06456953642384106\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph3-2.html",
    "href": "posts/graph3-2.html",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph3-2.html#load-dataset",
    "href": "posts/graph3-2.html#load-dataset",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n2023-04-06 18:27:51.222436: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\n\n\nx_train = x_train.astype('float32') / 255. #0~1사이로 정규화 하기 위하여\nx_test = x_test.astype('float32') / 255.\n\nprint (x_train.shape)\nprint (x_test.shape)\n\n(60000, 28, 28)\n(10000, 28, 28)\n\n\n\nfrom matplotlib import pyplot as plt\n\n\nclasses = {\n    0:\"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\", \n}\n\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(1, n, i + 1)\n    plt.imshow(x_test[i])\n    plt.title(classes[y_test[i]])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n# plt.savefig(\"TrainingSet.png\")"
  },
  {
    "objectID": "posts/graph3-2.html#create-autoencoder",
    "href": "posts/graph3-2.html#create-autoencoder",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Create Autoencoder",
    "text": "Create Autoencoder\n\nfrom tensorflow.keras.layers import Flatten, Conv2D, Dropout, MaxPooling2D, UpSampling2D, Input\n\n\nfrom tensorflow.keras import Model\n\n\n인코더/디코더\n\ninput_img = Input(shape=(28, 28, 1))\n\n#인코딩\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)      # 출력값 28x28x16\nx = MaxPooling2D((2, 2), padding='same')(x)                               # 출력값 14x14x16\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)               # 출력값 14x14x8\nx = MaxPooling2D((2, 2), padding='same')(x)                               # 출력값 7x7x8\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)               # 출력값 7x7x8\nencoded = MaxPooling2D((2, 2), padding='same')(x)                         # 출력값 4x4x8\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\n#디코딩\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)         # 출력값 4x4x8\nx = UpSampling2D((2, 2))(x)                                               # 출력값 8x8x8\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\n\n\nModel(input_img, encoded).summary()\n\nModel: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 28, 28, 16)        160       \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 14, 14, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_8 (Conv2D)           (None, 14, 14, 8)         1160      \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 7, 7, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_9 (Conv2D)           (None, 7, 7, 8)           584       \n                                                                 \n max_pooling2d_5 (MaxPooling  (None, 4, 4, 8)          0         \n 2D)                                                             \n                                                                 \n=================================================================\nTotal params: 1,904\nTrainable params: 1,904\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n손실함수/옵티마이저\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n학습\n\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\nEpoch 1/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0399 - val_loss: 0.0086\nEpoch 2/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0085 - val_loss: 0.0085\nEpoch 3/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0085 - val_loss: 0.0085\nEpoch 4/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 5/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 6/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0083\nEpoch 7/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0083 - val_loss: 0.0083\nEpoch 8/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0082 - val_loss: 0.0083\nEpoch 9/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0082 - val_loss: 0.0082\nEpoch 10/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 11/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 12/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 13/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0081 - val_loss: 0.0081\nEpoch 14/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 15/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 16/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 17/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 18/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 19/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 20/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 21/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 22/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 23/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 24/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 25/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 26/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 27/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 28/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 29/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 30/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 31/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 32/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0080 - val_loss: 0.0080\nEpoch 33/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 34/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 35/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 36/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 37/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 38/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 39/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 40/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 41/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 42/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 43/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 44/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 45/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 46/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 47/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 48/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 49/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 50/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\n\n\n&lt;keras.callbacks.History at 0x7f27e4110970&gt;\n\n\n\nautoencoder.save(\"./data/Batch50.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./data/Batch50.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/Batch50.p/assets\n\n\n\n\n예측\n\n예시1\n\n\nfrom tensorflow.keras.models import load_model\n\n\nautoencoder_first = load_model(\"./data/Batch50.p\")\n\n\ndecoded_imgs = autoencoder_first.predict(x_test)\n\nn = 6\nplt.figure(figsize=(20, 7))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)  #X축 숨기기\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n313/313 [==============================] - 0s 1ms/step\n\n\n\n\n\n\n\n\n\n\n첫 줄 우너본 이미지, 두번째 줄 재구성된 이미지\n\n\n\n예시2\n\nfrom tensorflow.keras.optimizers import Adam\n\n\nautoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy')\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=10,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\nEpoch 1/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 2/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 3/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0080\nEpoch 4/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 5/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 6/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 7/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 8/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 9/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\nEpoch 10/10\n469/469 [==============================] - 11s 23ms/step - loss: 0.0079 - val_loss: 0.0079\n\n\n&lt;keras.callbacks.History at 0x7f272ba11550&gt;\n\n\nepochs 위에건 50.. 넘 오래걸려서 10개로 줄여보자\n\nautoencoder.save(\"./data/Batch100.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./data/Batch100.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/Batch100.p/assets\n\n\n\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n\n313/313 [==============================] - 1s 2ms/step\n\n\n\n\n\n\n\n\n\n아담으로 5번 한거랑 우에서 50번한거랑 비슷하게 나왔땅"
  },
  {
    "objectID": "posts/graph3-2.html#embeddingst-sne사용",
    "href": "posts/graph3-2.html#embeddingst-sne사용",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Embeddings(T-SNE사용)",
    "text": "Embeddings(T-SNE사용)\n- T-SNE: 2차원 평면에서 이미지 인코딩된 버전 표현\n\nembeddings = Model(input_img, Flatten()(encoded)).predict(x_test)\n\n313/313 [==============================] - 0s 815us/step\n\n\n\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n\ntsne = TSNE(n_components=2)\n\n\nemb2d = tsne.fit_transform(embeddings)\n\n\nx,y = np.squeeze(emb2d[:, 0]), np.squeeze(emb2d[:, 1])\n\n\nnp.squeeze(emb2d[:, 0])는 emb2d의 모든 행에서 첫 번째 열만 선택하여 1차원 배열로 만드는 함수. 이를 통해 2차원 배열의 차원을 축소시키고, x좌표 값을 얻을 수 있다.\nnp.squeeze(emb2d[:, 1])는 emb2d의 모든 행에서 두 번째 열만 선택하여 1차원 배열로 만드는 함수. 이를 통해 2차원 배열의 차원을 축소시키고, y좌표 값을 얻을 수 있다.\n\n\nimport pandas as pd\n\n\nfrom matplotlib.cm import tab10\n\n\nsummary =  pd.DataFrame({\"x\": x, \"y\": y, \"target\": y_test, \"size\": 10})\n\nplt.figure(figsize=(10,8))\n\nfor key, sel in summary.groupby(\"target\"):\n    plt.scatter(sel[\"x\"], sel[\"y\"], s=10, color=tab10.colors[key], label=classes[key])\n    \nplt.legend()\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n샘플이 속한 클래스에 따라서 색상이 지정\n서로 다른 의류의 클러스터링 명확\n\n\n단점: 학습된 이미지를 정확히 재생성하고 일반화하지 않는 경향"
  },
  {
    "objectID": "posts/graph3-2.html#denoising",
    "href": "posts/graph3-2.html#denoising",
    "title": "CH3. 비지도 그래프 학습(오토인코더)",
    "section": "Denoising",
    "text": "Denoising\n- 노이즈 제거 오토인코더 : 다양한 강도의 노이즈를 사용해 손상된 입력에 대해 노이즈 없는 답안 사용\n\nfrom tensorflow.keras.layers import GaussianNoise\n\n\ninput_img = Input(shape=(28, 28, 1))\n\nnoisy_input = GaussianNoise(0.1)(input_img)\n\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(noisy_input)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nnoisy_autoencoder = Model(input_img, decoded)\n\n\n손상된 입력을 사용해 네트워크 훈련\n출력에는 노이즈 없는 이미지 사용\nGaussianNoise 학습 중에 확률적 노이즈 추가\n\n\nModel(input_img, decoded).summary()\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n gaussian_noise (GaussianNoi  (None, 28, 28, 1)        0         \n se)                                                             \n                                                                 \n conv2d_14 (Conv2D)          (None, 28, 28, 16)        160       \n                                                                 \n max_pooling2d_6 (MaxPooling  (None, 14, 14, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_15 (Conv2D)          (None, 14, 14, 8)         1160      \n                                                                 \n max_pooling2d_7 (MaxPooling  (None, 7, 7, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_16 (Conv2D)          (None, 7, 7, 8)           584       \n                                                                 \n max_pooling2d_8 (MaxPooling  (None, 4, 4, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_17 (Conv2D)          (None, 4, 4, 8)           584       \n                                                                 \n up_sampling2d_6 (UpSampling  (None, 8, 8, 8)          0         \n 2D)                                                             \n                                                                 \n conv2d_18 (Conv2D)          (None, 8, 8, 8)           584       \n                                                                 \n up_sampling2d_7 (UpSampling  (None, 16, 16, 8)        0         \n 2D)                                                             \n                                                                 \n conv2d_19 (Conv2D)          (None, 14, 14, 16)        1168      \n                                                                 \n up_sampling2d_8 (UpSampling  (None, 28, 28, 16)       0         \n 2D)                                                             \n                                                                 \n conv2d_20 (Conv2D)          (None, 28, 28, 1)         145       \n                                                                 \n=================================================================\nTotal params: 4,385\nTrainable params: 4,385\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnoisy_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\nnoisy_autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/noisy_autoencoder')])\n\nEpoch 1/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0201\nEpoch 2/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0193\nEpoch 3/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0180\nEpoch 4/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0162\nEpoch 5/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0157\nEpoch 6/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0150\nEpoch 7/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0135\nEpoch 8/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0129\nEpoch 9/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0123\nEpoch 10/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0111\nEpoch 11/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0109\nEpoch 12/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0103\nEpoch 13/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0098\nEpoch 14/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0096\nEpoch 15/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0094\nEpoch 16/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0094\nEpoch 17/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0090\nEpoch 18/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0088\nEpoch 19/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 20/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0088\nEpoch 21/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0087\nEpoch 22/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0087\nEpoch 23/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 24/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 25/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 26/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 27/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 28/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 29/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0086\nEpoch 30/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 31/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 32/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 33/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 34/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 35/50\n469/469 [==============================] - 11s 24ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 36/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 37/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 38/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 39/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 40/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 41/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 42/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 43/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 44/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 45/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 46/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 47/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 48/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\nEpoch 49/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0084\nEpoch 50/50\n469/469 [==============================] - 11s 23ms/step - loss: 0.0084 - val_loss: 0.0085\n\n\n&lt;keras.callbacks.History at 0x7f27900e2670&gt;\n\n\n\nautoencoder.save(\"./data/DenoisingAutoencoder.p\")\n\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./data/DenoisingAutoencoder.p/assets\n\n\nINFO:tensorflow:Assets written to: ./data/DenoisingAutoencoder.p/assets\n\n\n\nnoise_factor = 0.1\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\nnp.clip 데이터 0과 1사이의 값으로 클리핑\n\n\ndecoded_imgs = autoencoder.predict(x_test_noisy)\n\ndecoded_imgs_denoised = noisy_autoencoder.predict(x_test_noisy)\n\nn = 6\nplt.figure(figsize=(20, 10))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(3, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Original\")\n    else:\n        ax.get_yaxis().set_visible(False)\n        \n    # Display reconstruction\n    ax = plt.subplot(3, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Vanilla Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n     \n    ax = plt.subplot(3, n, i + 2*n)\n    plt.imshow(decoded_imgs_denoised[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Denoising Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n    \n        \nplt.show()\n\n313/313 [==============================] - 0s 1ms/step\n313/313 [==============================] - 0s 1ms/step\n\n\n\n\n\n\n\n\n\n\ndecoded_imgs = noisy_autoencoder.predict(x_test_noisy)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n313/313 [==============================] - 0s 1ms/step"
  },
  {
    "objectID": "posts/graph8-3.html",
    "href": "posts/graph8-3.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(0.4)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8-3.html#커뮤니티-감지",
    "href": "posts/graph8-3.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(0.4)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n1049192      0\n0          169\n1048885      2\n1            2\n1048746    106\n          ... \n1048822    155\n1049918    104\n1050153     57\n1048617    158\n1048898     98\nLength: 1050211, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n0      30837\n94     14755\n118    14020\n60     13830\n32     11859\n       ...  \n86       721\n8        720\n79       716\n113      693\n125      686\nLength: 173, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)"
  },
  {
    "objectID": "posts/graph8-3.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8-3.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(0.4)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.47it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7349397590361446\nRecall: 0.15996503496503497\nF1-Score: 0.26274228284278534\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6856264411990777\nRecall: 0.7797202797202797\nF1-Score: 0.7296523517382413\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5737704918032787\nRecall: 0.030594405594405596\nF1-Score: 0.05809128630705394\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.609375\nRecall: 0.03409090909090909\nF1-Score: 0.06456953642384106\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/9999.html",
    "href": "posts/9999.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/9999.html#imports",
    "href": "posts/9999.html#imports",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/9999.html#read-and-define-data",
    "href": "posts/9999.html#read-and-define-data",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/9999.html#embedding",
    "href": "posts/9999.html#embedding",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/9999.html#learn",
    "href": "posts/9999.html#learn",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/9999.html#evaluate",
    "href": "posts/9999.html#evaluate",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/9999.html#read-and-define-data-1",
    "href": "posts/9999.html#read-and-define-data-1",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(교수님)",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\nlrnr2, _,_,_ = anal(down_sample_textbook(our_sampling1(df)),n_estimators=100)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  3.01it/s]"
  },
  {
    "objectID": "posts/graph basic.html",
    "href": "posts/graph basic.html",
    "title": "CH1. graph basic",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph basic.html#간선가중그래프-gvew",
    "href": "posts/graph basic.html#간선가중그래프-gvew",
    "title": "CH1. graph basic",
    "section": "간선가중그래프 \\(G=(V,E,w)\\)",
    "text": "간선가중그래프 \\(G=(V,E,w)\\)\n\n\\(V\\): 노드의 집합, \\(E\\): 간선의 집합, \\(w: E \\to \\mathbb{R}\\): 각 간선을 실수 \\(e \\in E\\) 가중값으로 대중시키는 가중함수"
  },
  {
    "objectID": "posts/graph basic.html#노드가중그래프-gvew",
    "href": "posts/graph basic.html#노드가중그래프-gvew",
    "title": "CH1. graph basic",
    "section": "노드가중그래프 \\(G=(V,E,w)\\)",
    "text": "노드가중그래프 \\(G=(V,E,w)\\)\n\n\\(V\\): 노드의 집합, \\(E\\): 간선의 집합, \\(w: V \\to \\mathbb{R}\\): 각 간선을 실수 \\(v \\in V\\) 가중값으로 대중시키는 가중함수"
  },
  {
    "objectID": "posts/graph basic.html#예제k-means",
    "href": "posts/graph basic.html#예제k-means",
    "title": "CH1. graph basic",
    "section": "예제(k-means)",
    "text": "예제(k-means)\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# 데이터셋 로드\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n\n# 모델 생성 및 학습\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(df)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\nKMeans(n_clusters=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, random_state=0)\n\n\n\n# 클러스터링 결과 시각화\ndf['cluster'] = kmeans.labels_\nplt.scatter(df['petal length (cm)'], df['petal width (cm)'], c=df['cluster'])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.show()\n\n\n\n\n\n\n\n\n- ref: [출처] 클러스터링(Clustering)이란? 클러스터링 특징, 종류, 예제 실습|작성자 리르시"
  },
  {
    "objectID": "posts/graph8(over-sampling)syn.html",
    "href": "posts/graph8(over-sampling)syn.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(under-sampling)",
    "section": "",
    "text": "import pandas as pd\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nimport pandas as pd\ndf = pd.read_csv(\"fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ndf[\"is_fraud\"].value_counts()\n\n0    208514\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\n# 기존 코드 (down)\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\nfrom sklearn.utils import resample\n\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_min_oversampled = resample(df_minority,\n                              n_samples=len(df_majority),\n                              replace=True,\n                              random_state=42)\n\ndf_oversampled = pd.concat([df_majority, df_min_oversampled])\n\nprint(df_oversampled.is_fraud.value_counts())\nG_over = build_graph_bipartite(df_oversampled)\n\npip install gran\n\nRequirement already satisfied: gran in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (0.0.1)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport sys\nsys.path.append('/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages')\n\n\nfrom gran import GRAN\nimport networkx as nx\n\nModuleNotFoundError: No module named 'gran'\n\n\n\nfrom gran import GRAN\nimport networkx as nx\n\n# 기존의 그래프 데이터셋\ngraphs = [...]\n# GRAN 모델 초기화\ngran = GRAN(device='cpu')\n# 그래프 데이터 증강\nnew_graphs = gran.generate(graphs)\n\n# 생성된 그래프 데이터 확인\nfor i, new_graph in enumerate(new_graphs):\n    # NetworkX 그래프 객체로 변환\n    nx_graph = gran.to_networkx(new_graph)\n    # 그래프 시각화\n    nx.draw(nx_graph, with_labels=True)\n​\n\nSyntaxError: invalid character in identifier (384620633.py, line 17)\n\n\n\npip install torch numpy scipy tqdm\n$ pip install git+https://github.com/snap-stanford/ogb.git\n$ pip install git+https://github.com/snap-stanford/graph-doc2vec.git\n$ pip install git+https://github.com/snap-stanford/GRAN.git\n\n\npip install torch numpy scipy tqdm\n\nRequirement already satisfied: torch in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (1.13.1)\nRequirement already satisfied: numpy in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (1.22.4)\nRequirement already satisfied: scipy in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (1.10.1)\nRequirement already satisfied: tqdm in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (4.65.0)\nRequirement already satisfied: typing_extensions in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from torch) (4.4.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install git+https://github.com/snap-stanford/ogb.git\n\nCollecting git+https://github.com/snap-stanford/ogb.git\n  Cloning https://github.com/snap-stanford/ogb.git to /tmp/pip-req-build-rvf1uik4\n  Running command git clone --filter=blob:none --quiet https://github.com/snap-stanford/ogb.git /tmp/pip-req-build-rvf1uik4\n  Resolved https://github.com/snap-stanford/ogb.git to commit a47b716f7e972f666eae9909ee0f922cd0f9d966\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: torch&gt;=1.6.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.13.1)\nRequirement already satisfied: numpy&gt;=1.16.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.22.4)\nRequirement already satisfied: tqdm&gt;=4.29.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (4.65.0)\nRequirement already satisfied: scikit-learn&gt;=0.20.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.2.2)\nRequirement already satisfied: pandas&gt;=0.24.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.3.5)\nRequirement already satisfied: six&gt;=1.12.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.16.0)\nRequirement already satisfied: urllib3&gt;=1.24.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from ogb==1.3.6) (1.26.15)\nCollecting outdated&gt;=0.2.0\n  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nCollecting littleutils\n  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: setuptools&gt;=44 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from outdated&gt;=0.2.0-&gt;ogb==1.3.6) (65.6.3)\nRequirement already satisfied: requests in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from outdated&gt;=0.2.0-&gt;ogb==1.3.6) (2.28.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas&gt;=0.24.0-&gt;ogb==1.3.6) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from pandas&gt;=0.24.0-&gt;ogb==1.3.6) (2022.7)\nRequirement already satisfied: scipy&gt;=1.3.2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;ogb==1.3.6) (1.10.1)\nRequirement already satisfied: joblib&gt;=1.1.1 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;ogb==1.3.6) (1.2.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from scikit-learn&gt;=0.20.0-&gt;ogb==1.3.6) (3.1.0)\nRequirement already satisfied: typing_extensions in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from torch&gt;=1.6.0-&gt;ogb==1.3.6) (4.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests-&gt;outdated&gt;=0.2.0-&gt;ogb==1.3.6) (2022.12.7)\nRequirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests-&gt;outdated&gt;=0.2.0-&gt;ogb==1.3.6) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from requests-&gt;outdated&gt;=0.2.0-&gt;ogb==1.3.6) (3.4)\nBuilding wheels for collected packages: ogb, littleutils\n  Building wheel for ogb (setup.py) ... done\n  Created wheel for ogb: filename=ogb-1.3.6-py3-none-any.whl size=78743 sha256=cc58571b3e5c903fd2395540d77d76740c2f810f3663639c9299e2a64236f590\n  Stored in directory: /tmp/pip-ephem-wheel-cache-8rpjzmhk/wheels/c1/20/5b/76ab6aa5c9588d05152dfc4a7088179e040f7db7498d771b56\n  Building wheel for littleutils (setup.py) ... done\n  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=ba71828ff5f9a53430843ca994d1df80eeeee554ed19b36f6ea962853d8aa708\n  Stored in directory: /home/coco/.cache/pip/wheels/6a/33/c4/0ef84d7f5568c2823e3d63a6e08988852fb9e4bc822034870a\nSuccessfully built ogb littleutils\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install git+https://github.com/snap-stanford/graph-doc2vec.git\n\nCollecting git+https://github.com/snap-stanford/graph-doc2vec.git\n  Cloning https://github.com/snap-stanford/graph-doc2vec.git to /tmp/pip-req-build-ms1szh1h\n  Running command git clone --filter=blob:none --quiet https://github.com/snap-stanford/graph-doc2vec.git /tmp/pip-req-build-ms1szh1h\nUsername for 'https://github.com': \n\n\n\npip install git+https://github.com/snap-stanford/GRAN.git"
  },
  {
    "objectID": "posts/graph8(logistic-amt+time+citypop+lat+merchlat).html",
    "href": "posts/graph8(logistic-amt+time+citypop+lat+merchlat).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+city_pop+lat+merch_lat-f1:0.985323)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\ntype(_df2)\n\npandas.core.frame.DataFrame\n\n\n\ndiff = _df2['lat'] - _df2['merch_lat']\nlatabs=abs(diff)\nprint(\"lat:\",abs(diff).mean())\ndiff2 = _df2['long'] - _df2['merch_long']\nlongabs=abs(diff2)\nprint(\"long:\",abs(diff2).mean())\n\nlat: 0.5002190204058765\nlong: 0.5004574515650185\n\n\n\n_df2 = _df2.assign(latabs=abs(_df2['lat'] - _df2['merch_lat']))\n\n\n_df2 = _df2.assign(longabs=abs(_df2['long'] - _df2['merch_long']))\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean,'latabs':np.mean, 'longabs':np.mean})\n\n\n\n\n\n\n\n\ncity_pop\namt\ntime\nlatabs\nlongabs\n\n\nis_fraud\n\n\n\n\n\n\n\n\n\n0\n83870.443845\n67.743047\n12.813152\n0.500202\n0.500468\n\n\n1\n96323.951715\n530.573492\n13.915917\n0.502055\n0.499343\n\n\n\n\n\n\n\n\n_df3=_df2[['amt','time','city_pop','lat','merch_lat','is_fraud']]\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[4.5000000e+01, 0.0000000e+00, 1.9390000e+03, 4.6230600e+01,\n        4.7034331e+01, 0.0000000e+00],\n       [9.4630000e+01, 0.0000000e+00, 2.1580000e+03, 4.0375000e+01,\n        4.0653382e+01, 0.0000000e+00],\n       [4.4540000e+01, 0.0000000e+00, 2.6910000e+03, 3.7993100e+01,\n        3.7162705e+01, 0.0000000e+00],\n       ...,\n       [6.0300000e+00, 1.6000000e+01, 5.2000000e+02, 4.2193900e+01,\n        4.2633354e+01, 0.0000000e+00],\n       [1.1694000e+02, 1.6000000e+01, 1.5830000e+03, 4.1182600e+01,\n        4.1400318e+01, 0.0000000e+00],\n       [6.8100000e+00, 1.6000000e+01, 1.6555600e+05, 3.4077000e+01,\n        3.3601468e+01, 0.0000000e+00]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9893541900127412\nPrecision: 0.9815505737898247\nRecall: 0.9893541900127412\nF1 score: 0.9853239587931848\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9893541900127412\nPrecision:0.5132529996838914\nRecall: 0.5018112950402922\nF1 score: 0.5016129588437686\n\n\n\n간단하게 생각해보면, 고객의 lat과 상점의 lat의 차이가 크다.. 그러면 사기거래일 가능성이 클 거 같은 느낌.?"
  },
  {
    "objectID": "posts/graph8(logistic-amt+time+lat+merchlat).html",
    "href": "posts/graph8(logistic-amt+time+lat+merchlat).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+lat+merch_lat-f1:0.98538)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\ntype(_df2)\n\npandas.core.frame.DataFrame\n\n\n\ndiff = _df2['lat'] - _df2['merch_lat']\nlatabs=abs(diff)\nprint(\"lat:\",abs(diff).mean())\ndiff2 = _df2['long'] - _df2['merch_long']\nlongabs=abs(diff2)\nprint(\"long:\",abs(diff2).mean())\n\nlat: 0.5002190204058765\nlong: 0.5004574515650185\n\n\n\n_df2 = _df2.assign(latabs=abs(_df2['lat'] - _df2['merch_lat']))\n\n\n_df2 = _df2.assign(longabs=abs(_df2['long'] - _df2['merch_long']))\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean,'latabs':np.mean, 'longabs':np.mean})\n\n\n\n\n\n\n\n\ncity_pop\namt\ntime\nlatabs\nlongabs\n\n\nis_fraud\n\n\n\n\n\n\n\n\n\n0\n83870.443845\n67.743047\n12.813152\n0.500202\n0.500468\n\n\n1\n96323.951715\n530.573492\n13.915917\n0.502055\n0.499343\n\n\n\n\n\n\n\n\n_df3=_df2[['amt','time','lat','merch_lat','is_fraud']]\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[ 45.      ,   0.      ,  46.2306  ,  47.034331,   0.      ],\n       [ 94.63    ,   0.      ,  40.375   ,  40.653382,   0.      ],\n       [ 44.54    ,   0.      ,  37.9931  ,  37.162705,   0.      ],\n       ...,\n       [  6.03    ,  16.      ,  42.1939  ,  42.633354,   0.      ],\n       [116.94    ,  16.      ,  41.1826  ,  41.400318,   0.      ],\n       [  6.81    ,  16.      ,  34.077   ,  33.601468,   0.      ]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9891776553121594\nPrecision: 0.9810129371194015\nRecall: 0.9891776553121594\nF1 score: 0.9850783784050309\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9891776553121594\nPrecision:0.4952274089672451\nRecall: 0.49934905923560957\nF1 score: 0.4972796937822675\n\n\n\n간단하게 생각해보면, 고객의 lat과 상점의 lat의 차이가 크다.. 그러면 사기거래일 가능성이 클 거 같은 느낌.?"
  },
  {
    "objectID": "posts/graph4-2.html",
    "href": "posts/graph4-2.html",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-2.html#utility-graph-plot-matrix",
    "href": "posts/graph4-2.html#utility-graph-plot-matrix",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "Utility graph plot matrix",
    "text": "Utility graph plot matrix\n\nimport matplotlib.pyplot as plt\n\ndef draw_graph(G, node_names={}, nodes_label=[], node_size=900):\n    pos_nodes = nx.spring_layout(G)\n    \n    col = {0:\"steelblue\",1:\"red\",2:\"green\"}\n    \n    colors = [col[x] for x in nodes_label]\n    \n    nx.draw_networkx(G, pos_nodes, with_labels=True, node_color=colors, node_size=node_size, edge_color='gray', \n            arrowsize=30)\n    \n    \n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    plt.show()"
  },
  {
    "objectID": "posts/graph4-2.html#라벨-전파-알고리즘",
    "href": "posts/graph4-2.html#라벨-전파-알고리즘",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "라벨 전파 알고리즘",
    "text": "라벨 전파 알고리즘\n\n주어진 노드의 라벨을 인접 노드 또는 해당 노드에서 도달할 가능성이 높은 노드로 전파\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\nnodes_label = [0 for x in range(len(G.nodes()))]\nnodes_label[0] = 1\nnodes_label[6] = 2\ndraw_graph(G, nodes_label=nodes_label, node_size=1200)\n\n\n\n\n\n\n\n\n\n0과 6에만 라벨이 지정됨\n라벨이 지정된 노드의 정보를 이용하여 다른 노드로 이동할 확률 계싼\n\n\n그림의 그래프에 대한 대각 차수 행렬\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD\n\narray([[2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0, 0, 0, 0],\n       [0, 0, 3, 0, 0, 0, 0, 0],\n       [0, 0, 0, 2, 0, 0, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0, 0],\n       [0, 0, 0, 0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0, 0, 2, 0],\n       [0, 0, 0, 0, 0, 0, 0, 2]])\n\n\n\n\n그림의 그래프에 대한 전이 행렬\n\nA = inv(D)*nx.to_numpy_matrix(G)\nA\n\nmatrix([[0.        , 0.5       , 0.5       , 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.33333333, 0.33333333, 0.        , 0.33333333, 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.5       , 0.        , 0.5       ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n         0.5       , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        , 0.33333333,\n         0.        , 0.33333333, 0.33333333],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.5       , 0.        , 0.5       ],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.5       , 0.5       , 0.        ]])\n\n\n\n\nLabel propagation implemenation\n\n행렬의 각 행은 샘플, 각 열은 특징을 나타냄\n\n\nimport numpy as np\nimport networkx as nx\nfrom numpy.linalg import inv\nfrom abc import ABCMeta, abstractmethod\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.utils.validation import check_is_fitted, _deprecate_positional_args\n\nclass GraphLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3):\n\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def predict(self, X):\n        \"\"\"Performs inductive inference across the model.\n        Parameters\n        ----------\n        X : A networkx array.\n            The data matrix.\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Predictions for input data.\n        \"\"\"\n        probas = self.predict_proba(X)\n        return self.classes_[np.argmax(probas, axis=1)].ravel()\n\n    def predict_proba(self, X):\n        \"\"\"Predict probability for each possible outcome.\n        Compute the probability estimates for each single node in X\n        and each possible outcome seen during training (categorical\n        distribution).\n        Parameters\n        ----------\n        X : A networkx array.\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes)\n            Normalized probability distributions across\n            class labels.\n        \"\"\"\n        check_is_fitted(self)\n        \n        return self.label_distributions_\n    \n    def _validate_data(self, X, y):\n        if not isinstance(X, nx.Graph):\n            raise ValueError(\"Input should be a networkX graph\")\n        if not len(y) == len(X.nodes()):\n            raise ValueError(\"Label data input shape should be equal to the number of nodes in the graph\")\n        return X, y\n    \n    @staticmethod\n    def build_label(x,classes):\n        tmp = np.zeros((classes))\n        tmp[x] = 1\n        return tmp\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        A = inv(D)*nx.to_numpy_matrix(G)\n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it &lt; self.max_iter & c_tool &gt; self.tol:\n            Y = A*Y_prev\n            #force labeled nodes\n            Y[labeled_index] = Y0[labeled_index]\n            \n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            \n            Y_prev = Y\n            \n        self.label_distributions_ = Y\n        return self\n\n\n이게 뭐시여\n\n\n\nLabel propagation execution\n\nglp = GraphLabelPropagation()\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 1\ny[6] = 0\nglp.fit(G,y)\ntmp = glp.predict(G)\nprint(glp.predict_proba(G))\n\ndraw_graph(G, nodes_label=tmp+1, node_size=1200)\n\n[[0.         1.        ]\n [0.05338542 0.90006109]\n [0.11845743 0.8081115 ]\n [0.31951678 0.553297  ]\n [0.553297   0.31951678]\n [0.8081115  0.11845743]\n [1.         0.        ]\n [0.90006109 0.05338542]]"
  },
  {
    "objectID": "posts/graph4-2.html#라벨-확산-알고리즘",
    "href": "posts/graph4-2.html#라벨-확산-알고리즘",
    "title": "CH4. 지도 그래프 학습(얕은 임베딩 방법)",
    "section": "라벨 확산 알고리즘",
    "text": "라벨 확산 알고리즘\n\n라벨 전파 방식의 한계인 초기 라벨링 극복하고자 만들어짐\n초기 라벨은 학습 과정엥서 수정이 불가. 각 반복에서 얻은 원래 값과 같아야함\n라벨이 지정된 제약을 완화해 라벨이 지정된 입력 노드가 교육 프로세스 중에 라벨 변경할 수 있도록 한다.\n\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\nnodes_label = [0 for x in range(len(G.nodes()))]\nnodes_label[0] = 1\nnodes_label[6] = 2\ndraw_graph(G, nodes_label=nodes_label, node_size=1200)\n\n\n\n\n\n\n\n\n\nDegree matrix\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD\n\narray([[2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0, 0, 0, 0],\n       [0, 0, 3, 0, 0, 0, 0, 0],\n       [0, 0, 0, 2, 0, 0, 0, 0],\n       [0, 0, 0, 0, 2, 0, 0, 0],\n       [0, 0, 0, 0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0, 0, 2, 0],\n       [0, 0, 0, 0, 0, 0, 0, 2]])\n\n\n\n\nNormalized graph Laplacian matrix\n\n라플라시안 행렬 위키백과\n\n\n\nfrom scipy.linalg import fractional_matrix_power\nD_inv = fractional_matrix_power(D, -0.5)\nL = D_inv*nx.to_numpy_matrix(G)*D_inv\nL\n\nmatrix([[0.        , 0.5       , 0.40824829, 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.5       , 0.        , 0.40824829, 0.        , 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.40824829, 0.40824829, 0.        , 0.40824829, 0.        ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.40824829, 0.        , 0.5       ,\n         0.        , 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.5       , 0.        ,\n         0.40824829, 0.        , 0.        ],\n        [0.        , 0.        , 0.        , 0.        , 0.40824829,\n         0.        , 0.40824829, 0.40824829],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.40824829, 0.        , 0.5       ],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.40824829, 0.5       , 0.        ]])\n\n\n\n\nLabel spreading implementation\n\nimport numpy as np\nimport networkx as nx\nfrom sklearn.preprocessing import normalize\nfrom scipy.linalg import fractional_matrix_power\nfrom sklearn.utils.multiclass import check_classification_targets\n\nclass GraphLabelSpreading(GraphLabelPropagation):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3, alpha=0.6):\n\n        self.alpha = alpha\n        super().__init__(max_iter, tol)\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        D_inv = np.matrix(fractional_matrix_power(D,-0.5))\n        L = D_inv*nx.to_numpy_matrix(G)*D_inv\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it &lt; self.max_iter & c_tool &gt; self.tol:\n            Y = self.alpha*(L*Y_prev)+((1-self.alpha)*Y0)\n\n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            Y_prev = Y\n        self.label_distributions_ = Y\n        return self\n\n\n\nLabel Spreading\n\ngls = GraphLabelSpreading(max_iter=1000)\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 1\ny[6] = 0\ngls.fit(G,y)\ntmp = gls.predict(G)\nprint(gls.predict_proba(G))\ndraw_graph(G, nodes_label=tmp+1, node_size=1200)\n\n[[0.00148824 0.50403871]\n [0.00148824 0.19630098]\n [0.00471728 0.18369265]\n [0.01591722 0.05001252]\n [0.05001252 0.01591722]\n [0.18369265 0.00471728]\n [0.50403871 0.00148824]\n [0.19630098 0.00148824]]\n\n\n\n\n\n\n\n\n\n\n라벨 전파 할당보다 라벨 확산 할당이 확률이 더 낮다.\n초기 라벨 할당의 영향이 정규화 매개 변수 \\(\\alpha\\)에 의해 가중되기 때문에"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html",
    "href": "posts/graph8(frac=0.4).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  2.62it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7451737451737451\nRecall: 0.16538131962296487\nF1-Score: 0.270687237026648\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.691699604743083\nRecall: 0.7497857754927164\nF1-Score: 0.7195723684210527\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.7142857142857143\nRecall: 0.029991431019708654\nF1-Score: 0.0575657894736842\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.64\nRecall: 0.027420736932305057\nF1-Score: 0.052588331963845526\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.32it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04336246478827236\nHomogeneity: 0.0383178531539123\nCompleteness: 0.05011351404941123\nV-Measure: 0.043428985282038965\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.11206093720015026\nHomogeneity: 0.10817496918905492\nCompleteness: 0.11635805522328385\nV-Measure: 0.11211739628609738\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.16558117117175825\nHomogeneity: 0.16557714823761863\nCompleteness: 0.16568764408717976\nV-Measure: 0.16563237773404058\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.1349652677966787\nHomogeneity: 0.1337881599748603\nCompleteness: 0.1362723387302234\nV-Measure: 0.13501882386803338\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph8(frac=0.4)_.html",
    "href": "posts/graph8(frac=0.4)_.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.4)_.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.4)_.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  2.62it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7451737451737451\nRecall: 0.16538131962296487\nF1-Score: 0.270687237026648\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.691699604743083\nRecall: 0.7497857754927164\nF1-Score: 0.7195723684210527\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.7142857142857143\nRecall: 0.029991431019708654\nF1-Score: 0.0575657894736842\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.64\nRecall: 0.027420736932305057\nF1-Score: 0.052588331963845526\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.32it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04336246478827236\nHomogeneity: 0.0383178531539123\nCompleteness: 0.05011351404941123\nV-Measure: 0.043428985282038965\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.11206093720015026\nHomogeneity: 0.10817496918905492\nCompleteness: 0.11635805522328385\nV-Measure: 0.11211739628609738\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.16558117117175825\nHomogeneity: 0.16557714823761863\nCompleteness: 0.16568764408717976\nV-Measure: 0.16563237773404058\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.1349652677966787\nHomogeneity: 0.1337881599748603\nCompleteness: 0.1362723387302234\nV-Measure: 0.13501882386803338\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph8.html",
    "href": "posts/graph8.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지"
  },
  {
    "objectID": "posts/graph8.html#네트워크-토폴로지",
    "href": "posts/graph8.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.17,  48.31,  76.29, 146.7 ])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.10159189882353903\n-0.8017506210033467\n\n\n\n음의 동류성\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8.html#커뮤니티-감지",
    "href": "posts/graph8.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n12    4019\n71    3999\n27    3743\n52    3739\n43    3679\n      ... \n32    1110\n93    1097\n49    1060\n26    1003\n33     892\nLength: 96, dtype: int64\n\n\n\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n2500부근에 형성되었고 ..\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n48    8.684864\n13    6.956522\n55    6.781235\n45    6.743257\n88    6.338616\n        ...   \n93    0.996377\n75    0.952381\n51    0.765957\n82    0.737265\n33    0.335946\nLength: 96, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 6\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(basic)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n(df_downsampled.is_fraud == 0).count()\n\n12012\n\n\n\nG_down.number_of_edges\n\n&lt;bound method Graph.number_of_edges of &lt;networkx.classes.graph.Graph object at 0x7f71b080c430&gt;&gt;\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\ntrain_graph.number_of_edges(), train_graph.number_of_nodes()\n\n(9351, 1624)\n\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.44it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.6953125\nRecall: 0.156140350877193\nF1-Score: 0.2550143266475645\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6813353566009105\nRecall: 0.787719298245614\nF1-Score: 0.7306753458096015\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5925925925925926\nRecall: 0.028070175438596492\nF1-Score: 0.05360134003350084\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.5833333333333334\nRecall: 0.02456140350877193\nF1-Score: 0.04713804713804714\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.25it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.0429862559854\nHomogeneity: 0.03813140300201337\nCompleteness: 0.049433212382250756\nV-Measure: 0.0430529554606017\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.09395128496638593\nHomogeneity: 0.08960753766432715\nCompleteness: 0.09886731281849871\nV-Measure: 0.09400995872350308\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17593048106009063\nHomogeneity: 0.17598531397290276\nCompleteness: 0.17597737533152563\nV-Measure: 0.17598134456268477\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.1362053730791375\nHomogeneity: 0.1349991253997398\nCompleteness: 0.1375429939044335\nV-Measure: 0.13625918760275774"
  },
  {
    "objectID": "posts/graph8(frac=0.3) yhat건든거.html",
    "href": "posts/graph8(frac=0.3) yhat건든거.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.3) yhat건든거.html#네트워크-토폴로지",
    "href": "posts/graph8(frac=0.3) yhat건든거.html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8(frac=0.3) yhat건든거.html#커뮤니티-감지",
    "href": "posts/graph8(frac=0.3) yhat건든거.html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n\n\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)"
  },
  {
    "objectID": "posts/graph8(frac=0.3) yhat건든거.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.3) yhat건든거.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\ndf.shape\n\n(318777, 23)\n\n\n\ndf_minority.shape\n\n(6006, 23)\n\n\n\ndf_majority.shape\n\n(312771, 23)\n\n\n\n6006 / 312771 \n\n0.019202547550763976\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.45it/s]\n\n\n\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(yhat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7236842105263158\nRecall: 0.1407849829351536\nF1-Score: 0.2357142857142857\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 0])\n\n\n\nyhat = rf.predict_proba(test_embeddings)\n\n\nyhat\n\narray([[0.457, 0.543],\n       [0.634, 0.366],\n       [0.609, 0.391],\n       ...,\n       [0.577, 0.423],\n       [0.59 , 0.41 ],\n       [0.557, 0.443]])\n\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph8(logistic+graph).html",
    "href": "posts/graph8(logistic+graph).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱+그래프)",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\ndf\n\nimport pandas as pd\ndf = pd.read_csv(\"fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\ndf[\"is_fraud\"].value_counts()\n\n0    208514\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.29it/s]\n\n\n\n\n\n\n\n_df2\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\ndf = _df2 \n\n\ndef build_graph_bipartite2(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled2 = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled2 = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled2.is_fraud.value_counts())\nG_down2 = build_graph_bipartite(df_downsampled2)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges2, test_edges2, train_labels2, test_labels2 = train_test_split(list(range(len(G_down2.edges))), \n                                                                      list(nx.get_edge_attributes(G_down2, \"label\").values()), \n                                                                      test_size=0.30, \n                                                                      random_state=42)\n\n\nedgs2 = list(G_down2.edges)\ntrain_graph2 = G_down2.edge_subgraph([edgs[x] for x in train_edges2]).copy()\ntrain_graph2.add_nodes_from(list(set(G_down2.nodes) - set(train_graph2.nodes)))\n\n\nnode2vec_train2 = Node2Vec(train_graph2, weight_key='weight')\nmodel_train2 = node2vec_train2.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.40it/s]\n\n\n\n\n\n\n\ntraing(graph), test(logistic)\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges2]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred2 = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred2)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred2)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred2)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.6554307116104869\nRecall: 0.09599561162918267\nF1-Score: 0.16746411483253587\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.7195710455764075\nRecall: 0.7361492046077893\nF1-Score: 0.7277657266811279\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.4666666666666667\nRecall: 0.01151947339550192\nF1-Score: 0.022483940042826552\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.5319148936170213\nRecall: 0.013713658804168952\nF1-Score: 0.026737967914438502"
  },
  {
    "objectID": "posts/graph8(logistic-amt+time+citypop).html",
    "href": "posts/graph8(logistic-amt+time+citypop).html",
    "title": "CH8. 신용카드 거래 분석(로지스틱amt+time+city_pop-f1:0.986655)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\nmerch_lat과 merch_long 은 상점의 위도 경도, 위의 lat과 long은 고객의 ??\ndob는 생년월일(date of birth)을 나타내는 변수\nunix_time 1970년 1월 1일 0시 0분 0초(UTC)부터 경과된 시간을 초(second) 단위로 표현하는 방법\nzip 우편번호\n`\n\nlen(set(_df2['city']))\n\n576\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean})\n\n\n\n\n\n\n\n\ncity_pop\namt\ntime\n\n\nis_fraud\n\n\n\n\n\n\n\n0\n83870.443845\n67.743047\n12.813152\n\n\n1\n96323.951715\n530.573492\n13.915917\n\n\n\n\n\n\n\n\n_df3=_df2[['amt','time','city_pop','is_fraud']]\n\n\n_df3.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 4 columns):\n #   Column    Non-Null Count   Dtype  \n---  ------    --------------   -----  \n 0   amt       651430 non-null  float64\n 1   time      651430 non-null  int64  \n 2   city_pop  651430 non-null  int64  \n 3   is_fraud  651430 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 24.9 MB\n\n\n\ndata=np.hstack([_df3.values[:,:]])\n\n\ndata\n\narray([[4.50000e+01, 0.00000e+00, 1.93900e+03, 0.00000e+00],\n       [9.46300e+01, 0.00000e+00, 2.15800e+03, 0.00000e+00],\n       [4.45400e+01, 0.00000e+00, 2.69100e+03, 0.00000e+00],\n       ...,\n       [6.03000e+00, 1.60000e+01, 5.20000e+02, 0.00000e+00],\n       [1.16940e+02, 1.60000e+01, 1.58300e+03, 0.00000e+00],\n       [6.81000e+00, 1.60000e+01, 1.65556e+05, 0.00000e+00]])\n\n\n\nX = data[:,:-1]\ny = data[:,-1]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n\nlr = LogisticRegression()\n\n\nlr.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ny_pred=lr.predict(X_test)\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1score = f1_score(y_test, y_pred, average='weighted')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision: {}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9902215126721213\nPrecision: 0.9831134944972946\nRecall: 0.9902215126721213\nF1 score: 0.9866547019260462\n\n\n\nacc= accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nf1score = f1_score(y_test, y_pred, average='macro')\nprint(\"Accuracy: {}\".format(acc))\nprint(\"Precision:{}\".format(precision))\nprint(\"Recall: {}\".format(recall))\nprint(\"F1 score: {}\".format(f1score))\n\nAccuracy: 0.9902215126721213\nPrecision:0.4957576316517569\nRecall: 0.49934201359322505\nF1 score: 0.49754336709114605\n\n\n\nf1 score가 엄청 커졌다. 이유가 뭘까? 처음에 city_pop에 대한 걸 생각했을때는 사기거래=0과 사기거래=1의 큰 차이가 없어보였는데 갑자기…"
  },
  {
    "objectID": "posts/230414.html",
    "href": "posts/230414.html",
    "title": "GML",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n_df = pd.read_csv(\"fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n_df2 = _df2.assign(time= list(map(lambda x: int(x.split(' ')[-1].split(':')[0]), _df2['trans_date_trans_time'])))\n\n\n_df2.shape\n\n(651430, 24)\n\n\n\n_df2.columns\n\nIndex(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n       'merch_lat', 'merch_long', 'is_fraud', 'time'],\n      dtype='object')\n\n\n\n_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 651430 entries, 3 to 1048574\nData columns (total 24 columns):\n #   Column                 Non-Null Count   Dtype  \n---  ------                 --------------   -----  \n 0   Unnamed: 0             651430 non-null  int64  \n 1   trans_date_trans_time  651430 non-null  object \n 2   cc_num                 651430 non-null  float64\n 3   merchant               651430 non-null  object \n 4   category               651430 non-null  object \n 5   amt                    651430 non-null  float64\n 6   first                  651430 non-null  object \n 7   last                   651430 non-null  object \n 8   gender                 651430 non-null  object \n 9   street                 651430 non-null  object \n 10  city                   651430 non-null  object \n 11  state                  651430 non-null  object \n 12  zip                    651430 non-null  int64  \n 13  lat                    651430 non-null  float64\n 14  long                   651430 non-null  float64\n 15  city_pop               651430 non-null  int64  \n 16  job                    651430 non-null  object \n 17  dob                    651430 non-null  object \n 18  trans_num              651430 non-null  object \n 19  unix_time              651430 non-null  int64  \n 20  merch_lat              651430 non-null  float64\n 21  merch_long             651430 non-null  float64\n 22  is_fraud               651430 non-null  int64  \n 23  time                   651430 non-null  int64  \ndtypes: float64(6), int64(6), object(12)\nmemory usage: 124.3+ MB\n\n\n\n_df2[\"is_fraud\"].value_counts()\n\n0    645424\n1      6006\nName: is_fraud, dtype: int64\n\n\n\n_df2[\"is_fraud\"].value_counts()/len(_df2)\n\n0    0.99078\n1    0.00922\nName: is_fraud, dtype: float64\n\n\n\n_df2.groupby(by=['is_fraud']).agg({'city_pop':np.mean,'amt':np.mean,'time':np.mean})\n\n\n\n\n\n\n\n\ncity_pop\namt\ntime\n\n\nis_fraud\n\n\n\n\n\n\n\n0\n83870.443845\n67.743047\n12.813152\n\n\n1\n96323.951715\n530.573492\n13.915917\n\n\n\n\n\n\n\n\n_df2.groupby(by=['category']).agg({'is_fraud':np.mean}).\n\n\n\n\n\n\n\n\n\n_df2.groupby(by=['time']).agg({'is_fraud':np.mean}).plot()\n\n\n\n\n\n\n\n\n\n그래프상 시간을 3등분 하거나 2등분 해서 적합시키면 좋을 거 같다.\n\n3등분: 20 ~ 04, 04 ~ 12, 12 ~ 20\n\n\n2등분: 06 ~ 18, 18 ~ 06\n\n사기거래와 사기거래가 아닌 그룹에서 데이터 범주가 차이가 나는걸 보면\n금액, 시간..\n\n\n_df2.groupby(by=['is_fraud']).agg({'trans_date_trans_time':np.mean})\n\nNameError: name '_df2' is not defined\n\n\n\n_df2.trans_date_trans_time[3].split(' ')[-1].split(':')[0]\n\n'0'\n\n\n\npd.to_datetime(['0:01','16:06'])\n\nDatetimeIndex(['2023-04-14 00:01:00', '2023-04-14 16:06:00'], dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  }
]